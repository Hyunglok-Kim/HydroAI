{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "b4de9652-f92c-4d19-a3c8-000d83301163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunglokkim/Insync/hkim@geol.sc.edu/Google_Drive /Users/hyunglokkim/cpuserver_data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    base_FP = '/Users/hyunglokkim/Insync/hkim@geol.sc.edu/Google_Drive'\n",
    "    cpuserver_data_FP = '/Users/hyunglokkim/cpuserver_data'\n",
    "elif platform.system() == 'Linux':\n",
    "    base_FP = '/home/subin/data'\n",
    "    cpuserver_data_FP = '/home/subin/cpuserver_data'\n",
    "else:\n",
    "    base_FP = '/data'\n",
    "    cpuserver_data_FP = '/data'\n",
    "sys.path.append(base_FP + '/python_modules')\n",
    "print(base_FP, cpuserver_data_FP)\n",
    "\n",
    "#hydroAI libs\n",
    "import HydroAI.ASCAT_TUW as hASCAT_TUW\n",
    "import HydroAI.Plot as hPlot\n",
    "import HydroAI.Data as hData\n",
    "import HydroAI.Grid as hGrid\n",
    "importlib.reload(hASCAT_TUW)\n",
    "importlib.reload(hPlot)\n",
    "importlib.reload(hData)\n",
    "importlib.reload(hGrid)\n",
    "\n",
    "# Ignore runtime warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define your directory where to save nc files\n",
    "nc_save_dir = cpuserver_data_FP + '/extracted_nc'\n",
    "output_dir  = cpuserver_data_FP + '/ASCAT/TUW/csv'  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97763f31-dccd-4fbf-bb0a-e6015a65b42c",
   "metadata": {},
   "source": [
    "## 1. Import ASCAT data from raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3d5a6e06-3595-40d9-b95d-0d96c13dc341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------------------+---------------+\n",
      "| Name           | Long Name                    | Units         |\n",
      "+================+==============================+===============+\n",
      "| lon            | location longitude           | degrees_east  |\n",
      "+----------------+------------------------------+---------------+\n",
      "| lat            | location latitude            | degrees_north |\n",
      "+----------------+------------------------------+---------------+\n",
      "| gpi            | grid point index             |               |\n",
      "+----------------+------------------------------+---------------+\n",
      "| cell           | cell number                  |               |\n",
      "+----------------+------------------------------+---------------+\n",
      "| land_flag      | land flag                    |               |\n",
      "+----------------+------------------------------+---------------+\n",
      "| committed_area | Committed soil moisture area |               |\n",
      "+----------------+------------------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "# get the WARP5 Grid Information (GI) nc file\n",
    "ASCAT_FP = cpuserver_data_FP + '/ASCAT/TUW'\n",
    "GI_file_path = os.path.join(ASCAT_FP, 'warp5_grid/TUW_WARP5_grid_info_2_3.nc')\n",
    "# check the variable names and their units\n",
    "hData.get_nc_variable_names_units(GI_file_path);\n",
    "\n",
    "gpi = hData.get_variable_from_nc(GI_file_path, 'gpi')\n",
    "lat = hData.get_variable_from_nc(GI_file_path, 'lat')\n",
    "lon = hData.get_variable_from_nc(GI_file_path, 'lon')\n",
    "cell = hData.get_variable_from_nc(GI_file_path, 'cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80c7ca6-8ecb-45d8-a30d-c925b91d78fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| Name                 | Long Name                               | Units                          |\n",
      "+======================+=========================================+================================+\n",
      "| row_size             | number of observations at this location |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| lon                  | location longitude                      | degrees_east                   |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| lat                  | location latitude                       | degrees_north                  |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| alt                  | vertical distance above the surface     | m                              |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| location_id          |                                         |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| location_description |                                         |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| time                 | time of measurement                     | days since 1900-01-01 00:00:00 |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| sm                   | surface soil moisture                   | percentage                     |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| sm_noise             | surface soil moisture noise             | percentage                     |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| dir                  | orbit direction                         |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| ssf                  | surface state flag                      |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| sat_id               | satellite id                            |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| proc_flag            | processing flag                         |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| corr_flag            | correction flag                         |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| conf_flag            | confidence flag                         |                                |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| slope40              | slope at 40 degree                      | dB/degree                      |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| slope40_noise        | slope at 40 degree noise                | dB/degree                      |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| curvature40          | curvature at 40 degree                  | dB/degree^2                    |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| curvature40_noise    | curvature at 40 degree noise            | dB/degree^2                    |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| sigma40              | backscatter at 40 degree                | dB                             |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n",
      "| sigma40_noise        | backscatter at 40 degree noise          | dB                             |\n",
      "+----------------------+-----------------------------------------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# ASCAT nc files\n",
    "nc_file_path = os.path.join(ASCAT_FP, 'h119')\n",
    "h119 = hData.get_file_list(nc_file_path, 'nc')\n",
    "nc_file_path = os.path.join(ASCAT_FP, 'h120')\n",
    "h120 = hData.get_file_list(nc_file_path, 'nc')\n",
    "ASCAT_file_path = h119+h120\n",
    "hData.get_nc_variable_names_units(ASCAT_file_path[0]);\n",
    "nof = len(ASCAT_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8804f2e-74ab-4574-be20-cbcbbd187b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cell_numbers = []\n",
    "index_map = {}\n",
    "\n",
    "for index, path in enumerate(ASCAT_file_path):\n",
    "    identifier = path.split('_')[-1].split('.')[0]\n",
    "    if identifier not in unique_cell_numbers:\n",
    "        unique_cell_numbers.append(identifier)\n",
    "        index_map[identifier] = [index]\n",
    "    else:\n",
    "        index_map[identifier].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "940d4e24-0c44-499c-8247-453172b80221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert fraction of days since 1900-01-01 00:00:00 UTC to local time\n",
    "def convert_to_local_time(df):\n",
    "    # Calculate UTC time\n",
    "    base_date = datetime(1900, 1, 1, 0, 0, 0)\n",
    "    utc_time = base_date + pd.to_timedelta(df['time'], unit='D')\n",
    "\n",
    "    # Calculate timezone offset based on longitude\n",
    "    timezone_offset_hours = df['lon'] / 15  # 15 degrees per hour\n",
    "    timezone_offset = pd.to_timedelta(timezone_offset_hours, unit='H')\n",
    "\n",
    "    # Convert to local time\n",
    "    local_time = utc_time + timezone_offset\n",
    "\n",
    "    # Convert local time back to fraction of days since 1900-01-01 00:00:00\n",
    "    local_time_fraction_days = (local_time - base_date) / timedelta(days=1)\n",
    "\n",
    "    return local_time_fraction_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aa0924-fa16-4d71-9b6f-7a65bf370061",
   "metadata": {},
   "source": [
    "## 1-1. Creat core csv files (should be updated yearly basis -- last updated Jun 1 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ca8d40-1449-4cc5-8f35-91fe2b550f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h119/H119_0032.nc\n",
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h120/H120_0032.nc\n"
     ]
    }
   ],
   "source": [
    "# Single file operator\n",
    "geo_vars = ['row_size', 'lon', 'lat', 'location_id']\n",
    "obs_vars = ['time', 'sm', 'sm_noise', 'ssf', 'dir', 'proc_flag', 'corr_flag', 'conf_flag','sigma40', 'sigma40_noise']\n",
    "\n",
    "# Extract geolocation related data for a specific k\n",
    "for k in [2]:#range(unique_cell_numbers):\n",
    "\n",
    "    h_file_index = index_map.get(unique_cell_numbers[k], [])\n",
    "    \n",
    "    obs_df_list = []\n",
    "    for kk in h_file_index:\n",
    "        t_ASCAT_file_name = ASCAT_file_path[kk]\n",
    "        t_cell_number = t_ASCAT_file_name[-7:-3] # cell #\n",
    "        #t_h_number = t_ASCAT_file_name[-11:-8]   # H119 or H120\n",
    "    \n",
    "        print(t_ASCAT_file_name)\n",
    "        # Extract geolocation related data\n",
    "        geo_df = pd.DataFrame(columns=geo_vars).drop(columns='row_size')\n",
    "        t_row_size = hData.get_variable_from_nc(t_ASCAT_file_name, 'row_size')\n",
    "        t_lon = hData.get_variable_from_nc(t_ASCAT_file_name, 'lon')\n",
    "        t_lat = hData.get_variable_from_nc(t_ASCAT_file_name, 'lat')\n",
    "        t_location_id = hData.get_variable_from_nc(t_ASCAT_file_name, 'location_id')\n",
    "        \n",
    "        # Extract observation related data\n",
    "        t_time = hData.get_variable_from_nc(t_ASCAT_file_name, 'time')\n",
    "        t_sm = hData.get_variable_from_nc(t_ASCAT_file_name, 'sm')\n",
    "        t_sm_noise = hData.get_variable_from_nc(t_ASCAT_file_name, 'sm_noise')\n",
    "        t_ssf = hData.get_variable_from_nc(t_ASCAT_file_name, 'ssf')\n",
    "        t_dir = hData.get_variable_from_nc(t_ASCAT_file_name, 'dir')\n",
    "        t_proc_flag = hData.get_variable_from_nc(t_ASCAT_file_name, 'proc_flag')\n",
    "        t_corr_flag = hData.get_variable_from_nc(t_ASCAT_file_name, 'corr_flag')\n",
    "        t_conf_flag = hData.get_variable_from_nc(t_ASCAT_file_name, 'conf_flag')\n",
    "        t_sigma40 = hData.get_variable_from_nc(t_ASCAT_file_name, 'sigma40')\n",
    "        t_sigma40_noise = hData.get_variable_from_nc(t_ASCAT_file_name, 'sigma40_noise')\n",
    "        \n",
    "        t_dir = t_dir.astype(float)  # Convert orbit_dir to float\n",
    "        t_row_size = t_row_size[~np.isnan(t_row_size)]\n",
    "        obs_partial_df = pd.DataFrame(index=np.arange(t_row_size.sum()), columns=obs_vars+['lon', 'lat', 'location_id'])\n",
    "        \n",
    "        # Save them to dataframes\n",
    "        row_start = np.zeros(len(t_row_size), dtype=int)\n",
    "        row_end = np.zeros(len(t_row_size), dtype=int)\n",
    "    \n",
    "        for i in range(len(t_row_size)):\n",
    "    \n",
    "            geo_df.loc[i, 'lon'] = t_lon[i]\n",
    "            geo_df.loc[i, 'lat'] = t_lat[i]\n",
    "            geo_df.loc[i, 'location_id'] = t_location_id[i]\n",
    "                \n",
    "            if i == 0:\n",
    "                row_start[0] = 0\n",
    "                row_end[0] = t_row_size[0]\n",
    "            else:\n",
    "                row_start[i] = row_end[i-1]\n",
    "                row_end[i] = row_start[i] + t_row_size[i]\n",
    "        \n",
    "            for var in obs_vars:\n",
    "                obs_partial_df.loc[row_start[i]:row_end[i]-1, var] = globals()[f't_{var}'][row_start[i]:row_end[i]]\n",
    "    \n",
    "            obs_partial_df.loc[row_start[i]:row_end[i]-1, 'lon'] = geo_df.loc[i, 'lon']\n",
    "            obs_partial_df.loc[row_start[i]:row_end[i]-1, 'lat'] = geo_df.loc[i, 'lat']\n",
    "            obs_partial_df.loc[row_start[i]:row_end[i]-1, 'location_id'] = geo_df.loc[i, 'location_id']\n",
    "             \n",
    "        # Append the partial DataFrame to the list\n",
    "        obs_df_list.append(obs_partial_df)\n",
    "    \n",
    "    # Concatenate all partial DataFrames into a single DataFrame\n",
    "    obs_df = pd.concat(obs_df_list, ignore_index=True)\n",
    "    obs_df['local_time'] = convert_to_local_time(obs_df)\n",
    "    obs_df.to_csv(os.path.join(output_dir, 'h119_h120_'+t_cell_number+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ca42369-f970-4de3-8bf2-dd17766e4ee9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/qjq3yrjn047588zvn9_fc2gc0000gn/T/ipykernel_17429/180234113.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "/var/folders/_4/qjq3yrjn047588zvn9_fc2gc0000gn/T/ipykernel_17429/180234113.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "/var/folders/_4/qjq3yrjn047588zvn9_fc2gc0000gn/T/ipykernel_17429/180234113.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "/var/folders/_4/qjq3yrjn047588zvn9_fc2gc0000gn/T/ipykernel_17429/180234113.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "/var/folders/_4/qjq3yrjn047588zvn9_fc2gc0000gn/T/ipykernel_17429/180234113.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "/var/folders/_4/qjq3yrjn047588zvn9_fc2gc0000gn/T/ipykernel_17429/180234113.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "/var/folders/_4/qjq3yrjn047588zvn9_fc2gc0000gn/T/ipykernel_17429/180234113.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h119/H119_0030.nc\n",
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h120/H120_0030.nc\n",
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h119/H119_0031.nc\n",
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h120/H120_0031.nc\n",
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h119/H119_1523.nc\n",
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h120/H120_1523.nc\n",
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h119/H119_1524.nc\n",
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/h120/H120_1524.nc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m     ranges[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m((n_processors \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m chunk_size, \u001b[38;5;28mlen\u001b[39m(unique_cell_numbers))\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Run the processing in parallel\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_processors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_cell_numbers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk_range\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mranges\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/site-packages/joblib/parallel.py:1703\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1703\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_abort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     \u001b[38;5;66;03m# Store the unconsumed tasks and terminate the workers if necessary\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/site-packages/joblib/parallel.py:1614\u001b[0m, in \u001b[0;36mParallel._abort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabort_everything\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[1;32m   1613\u001b[0m     ensure_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_backend\n\u001b[0;32m-> 1614\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabort_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/site-packages/joblib/_parallel_backends.py:620\u001b[0m, in \u001b[0;36mLokyBackend.abort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabort_everything\u001b[39m(\u001b[38;5;28mself\u001b[39m, ensure_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ensure_ready:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/site-packages/joblib/executor.py:75\u001b[0m, in \u001b[0;36mMemmappingExecutor.terminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m, kill_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkill_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# When workers are killed in a brutal manner, they cannot execute the\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# finalizer of their shared memmaps. The refcount of those memmaps may\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# be off by an unknown number, so instead of decref'ing them, we force\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# with allow_non_empty=True but if we can't, it will be clean up later\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# on by the resource_tracker.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_resize_lock:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:1303\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;66;03m# This locks avoids concurrent join if the interpreter\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;66;03m# is shutting down.\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _global_shutdown_lock:\n\u001b[0;32m-> 1303\u001b[0m         \u001b[43mexecutor_manager_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m         _threads_wakeups\u001b[38;5;241m.\u001b[39mpop(executor_manager_thread, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/hydroai/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parallelize the execution of the loop using joblib\n",
    "# Function to process data for a specific range of k values\n",
    "def process_data(k_range, unique_cell_numbers):\n",
    "    for k in k_range:\n",
    "        h_file_index = index_map.get(unique_cell_numbers[k], [])\n",
    "\n",
    "        obs_df_list = []\n",
    "        for kk in h_file_index:\n",
    "            t_ASCAT_file_name = ASCAT_file_path[kk]\n",
    "            t_cell_number = t_ASCAT_file_name[-7:-3]  # cell #\n",
    "            #t_h_number = t_ASCAT_file_name[-11:-8]   # H119 or H120\n",
    "\n",
    "            print(t_ASCAT_file_name)\n",
    "            # Extract geolocation related data\n",
    "            geo_df = pd.DataFrame(columns=geo_vars).drop(columns='row_size')\n",
    "            \n",
    "            t_row_size = hData.get_variable_from_nc(t_ASCAT_file_name, 'row_size')\n",
    "            t_lon = hData.get_variable_from_nc(t_ASCAT_file_name, 'lon')\n",
    "            t_lat = hData.get_variable_from_nc(t_ASCAT_file_name, 'lat')\n",
    "            t_location_id = hData.get_variable_from_nc(t_ASCAT_file_name, 'location_id')\n",
    "            \n",
    "            # Extract observation related data\n",
    "            for var in obs_vars:\n",
    "                globals()[f't_{var}'] = hData.get_variable_from_nc(t_ASCAT_file_name, var)\n",
    "                \n",
    "            t_time = hData.get_variable_from_nc(t_ASCAT_file_name, 'time')\n",
    "            t_sm = hData.get_variable_from_nc(t_ASCAT_file_name, 'sm')\n",
    "            t_sm_noise = hData.get_variable_from_nc(t_ASCAT_file_name, 'sm_noise')\n",
    "            t_ssf = hData.get_variable_from_nc(t_ASCAT_file_name, 'ssf')\n",
    "            t_dir = hData.get_variable_from_nc(t_ASCAT_file_name, 'dir')\n",
    "            t_proc_flag = hData.get_variable_from_nc(t_ASCAT_file_name, 'proc_flag')\n",
    "            t_corr_flag = hData.get_variable_from_nc(t_ASCAT_file_name, 'corr_flag')\n",
    "            t_conf_flag = hData.get_variable_from_nc(t_ASCAT_file_name, 'conf_flag')\n",
    "            t_sigma40 = hData.get_variable_from_nc(t_ASCAT_file_name, 'sigma40')\n",
    "            t_sigma40_noise = hData.get_variable_from_nc(t_ASCAT_file_name, 'sigma40_noise')\n",
    "            \n",
    "            t_dir = t_dir.astype(float)  # Convert orbit_dir to float\n",
    "            t_row_size = t_row_size[~np.isnan(t_row_size)]\n",
    "            obs_partial_df = pd.DataFrame(index=np.arange(t_row_size.sum()), columns=obs_vars+['lon', 'lat', 'location_id'])\n",
    "\n",
    "            # Save them to dataframes\n",
    "            row_start = np.zeros(len(t_row_size), dtype=int)\n",
    "            row_end = np.zeros(len(t_row_size), dtype=int)\n",
    "\n",
    "            for i in range(len(t_row_size)):\n",
    "                geo_df.loc[i, 'lon'] = t_lon[i]\n",
    "                geo_df.loc[i, 'lat'] = t_lat[i]\n",
    "                geo_df.loc[i, 'location_id'] = t_location_id[i]\n",
    "\n",
    "                if i == 0:\n",
    "                    row_start[0] = 0\n",
    "                    row_end[0] = t_row_size[0]\n",
    "                else:\n",
    "                    row_start[i] = row_end[i-1]\n",
    "                    row_end[i] = row_start[i] + t_row_size[i]\n",
    "\n",
    "                for var in obs_vars:\n",
    "                    obs_partial_df.loc[row_start[i]:row_end[i]-1, var] = globals()[f't_{var}'][row_start[i]:row_end[i]]\n",
    "\n",
    "                obs_partial_df.loc[row_start[i]:row_end[i]-1, 'lon'] = geo_df.loc[i, 'lon']\n",
    "                obs_partial_df.loc[row_start[i]:row_end[i]-1, 'lat'] = geo_df.loc[i, 'lat']\n",
    "                obs_partial_df.loc[row_start[i]:row_end[i]-1, 'location_id'] = geo_df.loc[i, 'location_id']\n",
    "\n",
    "            # Append the partial DataFrame to the list\n",
    "            obs_df_list.append(obs_partial_df)\n",
    "\n",
    "        # Concatenate all partial DataFrames into a single DataFrame\n",
    "        obs_df = pd.concat(obs_df_list, ignore_index=True)\n",
    "        obs_df['local_time'] = convert_to_local_time(obs_df)\n",
    "        #obs_df.to_csv(os.path.join(output_dir, 't_h119_h120_' + t_cell_number + '.csv'))\n",
    "        obs_df.to_csv(os.path.join(output_dir, 'h119_h120_' + t_cell_number + '.csv'))\n",
    "\n",
    "# Number of processors to use\n",
    "n_processors = 48\n",
    "\n",
    "# Divide the range into chunks based on the number of processors\n",
    "chunk_size = len(unique_cell_numbers) // n_processors\n",
    "ranges = [range(i * chunk_size, (i + 1) * chunk_size) for i in range(n_processors)]\n",
    "\n",
    "# If there are remaining elements, add them to the last chunk\n",
    "if len(unique_cell_numbers) % n_processors != 0:\n",
    "    ranges[-1] = range((n_processors - 1) * chunk_size, len(unique_cell_numbers))\n",
    "\n",
    "# Run the processing in parallel\n",
    "Parallel(n_jobs=n_processors)(delayed(process_data)(k_range, unique_cell_numbers) for k_range in ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "0353a31b-dd91-4a74-97f7-46b7be5e75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_3d_object_array(x, y, z):\n",
    "    \"\"\"\n",
    "    Create a 3D NumPy array that can store list data, with the specified shape (x, y, z).\n",
    "    \n",
    "    Parameters:\n",
    "    x (int): Size of the first dimension.\n",
    "    y (int): Size of the second dimension.\n",
    "    z (int): Size of the third dimension.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: 3D NumPy array of shape (x, y, z) with dtype=object.\n",
    "    \"\"\"\n",
    "    # Create an empty array with the given shape and dtype=object\n",
    "    obj_array = np.empty((x, y, z), dtype=object)\n",
    "    \n",
    "    # Initialize each element to an empty list\n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            for k in range(z):\n",
    "                obj_array[i, j, k] = []\n",
    "    \n",
    "    return obj_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "ea84c966-343f-4990-b0e8-db2e7eab3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lon, target_lat = hGrid.generate_lon_lat_eqdgrid(0.25)\n",
    "start_year = 2007\n",
    "end_year   = 2023\n",
    "num_days   = (datetime(end_year+1,1,1) - datetime(start_year,1,1)).days\n",
    "cutoff_datetime = pd.to_datetime(str(end_year+1)+'-01-01 00:00:00')\n",
    "\n",
    "csv_folder = os.path.join(ASCAT_FP, 'csv')\n",
    "csv_file_path = hData.get_file_list(csv_folder, 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1e96291e-b381-447c-af82-3d844aac76d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hyunglokkim/cpuserver_data/ASCAT/TUW/csv/h119_h120_0031.csv\n"
     ]
    }
   ],
   "source": [
    "i = 1 \n",
    "\n",
    "orbit = 0 # 0: ascending (pm; 21:30) / 1: descending (am; 9:30)\n",
    "\n",
    "t_csv_file_name = csv_file_path[i]\n",
    "print(t_csv_file_name)\n",
    "t_cell_number   = t_csv_file_name[-8:-4]\n",
    "t_lat           = pd.read_csv(t_csv_file_name, usecols=['lat'])\n",
    "t_lon           = pd.read_csv(t_csv_file_name, usecols=['lon'])\n",
    "t_local_time    = pd.read_csv(t_csv_file_name, usecols=['local_time'])\n",
    "t_time    = pd.read_csv(t_csv_file_name, usecols=['time'])\n",
    "t_location_id   = pd.read_csv(t_csv_file_name, usecols=['location_id'])\n",
    "t_sm            = pd.read_csv(t_csv_file_name, usecols=['sm'])\n",
    "t_dir           = pd.read_csv(t_csv_file_name, usecols=['dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "7f906a96-a836-4073-83bb-137a4fd5b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           local_time                   datetime   doy\n",
      "0        39085.417395 2007-01-05 10:01:02.922379     5\n",
      "1        39085.486970 2007-01-05 11:41:14.172373     5\n",
      "2        39086.402920 2007-01-06 09:40:12.296372     6\n",
      "3        39086.816049 2007-01-06 19:35:06.672348     6\n",
      "4        39086.885733 2007-01-06 21:15:27.297345     6\n",
      "...               ...                        ...   ...\n",
      "8016948  45291.890410 2024-01-02 21:22:11.461385  6211\n",
      "8016949  45292.423961 2024-01-03 10:10:30.212389  6212\n",
      "8016950  45292.493579 2024-01-03 11:50:45.212392  6212\n",
      "8016951  45292.768861 2024-01-03 18:27:09.587389  6212\n",
      "8016952  45293.409529 2024-01-04 09:49:43.337398  6213\n",
      "\n",
      "[8016953 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert fractional days to datetime\n",
    "base_date = datetime(1900, 1, 1)\n",
    "t_local_time['datetime'] = t_local_time['local_time'].apply(lambda x: base_date + timedelta(days=x))\n",
    "\n",
    "# Define the base year for DOY calculation\n",
    "base_year = start_year\n",
    "\n",
    "# Function to calculate DOY with baseline year\n",
    "def calculate_doy(dt, base_year):\n",
    "    year_start = datetime(base_year, 1, 1)\n",
    "    doy = (dt - year_start).days + 1\n",
    "    return doy\n",
    "\n",
    "# Apply the function to calculate DOY\n",
    "t_local_time['doy'] = t_local_time['datetime'].apply(lambda x: calculate_doy(x, base_year))\n",
    "\n",
    "# Display the dataframe\n",
    "print(t_local_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "e02e125e-a35a-497b-ba1c-58fc0fde19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ll_idx = hData.find_closest_index(target_lon, target_lat, [np.min(t_lon), np.min(t_lat)])\n",
    "t_lr_idx = hData.find_closest_index(target_lon, target_lat, [np.max(t_lon), np.min(t_lat)])\n",
    "t_ul_idx = hData.find_closest_index(target_lon, target_lat, [np.min(t_lon), np.max(t_lat)])\n",
    "t_ur_idx = hData.find_closest_index(target_lon, target_lat, [np.max(t_lon), np.max(t_lat)])\n",
    "\n",
    "t_target_frame = create_3d_object_array(t_ll_idx[0]-t_ul_idx[0]+1, t_lr_idx[1]-t_ll_idx[1]+1, num_days)\n",
    "t_target_lon   = target_lon[t_ul_idx[0]:t_ll_idx[0]+1, t_ll_idx[1]:t_lr_idx[1]+1]\n",
    "t_target_lat   = target_lat[t_ul_idx[0]:t_ll_idx[0]+1, t_ll_idx[1]:t_lr_idx[1]+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "1752c2f5-de53-4626-afb3-7aa02749f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_unique_location_id = pd.DataFrame(np.unique(t_location_id['location_id']).astype('int'), columns=['unique_location_id'])\n",
    "for ti in range(len(t_unique_location_id)):\n",
    "    t_lat_idx = hData.find_closest_index(t_target_lon, t_target_lat, [lon[t_unique_location_id.unique_location_id[ti]], lat[t_unique_location_id.unique_location_id[ti]]])[0]\n",
    "    t_lon_idx = hData.find_closest_index(t_target_lon, t_target_lat, [lon[t_unique_location_id.unique_location_id[ti]], lat[t_unique_location_id.unique_location_id[ti]]])[1]\n",
    "    t_unique_location_id.loc[ti, 'lat_idx'] = t_lat_idx\n",
    "    t_unique_location_id.loc[ti, 'lon_idx'] = t_lon_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e75d9-5f64-434d-9422-cdbbc60cfda0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██▉                             | 369735/4036608 [01:31<15:27, 3953.96it/s]"
     ]
    }
   ],
   "source": [
    "t_data_filter = (t_local_time['datetime'] < cutoff_datetime) & (t_dir['dir'] == orbit)\n",
    "\n",
    "tt_sm = t_sm.loc[t_data_filter].reset_index(drop=True)\n",
    "tt_local_time = t_local_time[t_data_filter].reset_index(drop=True)\n",
    "tt_location_id = t_location_id[t_data_filter].reset_index(drop=True)\n",
    "\n",
    "si = 0\n",
    "for si in tqdm(range(len(tt_sm))):\n",
    "    tt_lat_idx = t_unique_location_id[t_unique_location_id['unique_location_id'] == tt_location_id.iloc[si][0]].lat_idx.iloc[0].astype('int')\n",
    "    tt_lon_idx = t_unique_location_id[t_unique_location_id['unique_location_id'] == tt_location_id.iloc[si][0]].lon_idx.iloc[0].astype('int')\n",
    "    tt_doy     = tt_local_time.iloc[si]['doy']\n",
    "    t_target_frame[tt_lat_idx, tt_lon_idx, tt_doy].append(tt_sm.iloc[si]['sm'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
