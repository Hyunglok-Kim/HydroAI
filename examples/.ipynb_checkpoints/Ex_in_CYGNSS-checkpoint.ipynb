{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea91ae38-33d5-47cf-9f55-6c0e48f13f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import platform\n",
    "import importlib\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    base_FP = '/Users/hyunglokkim/Insync/hkim@geol.sc.edu/Google_Drive'\n",
    "    cpuserver_data_FP = '/Users/hyunglokkim/cpuserver_data'\n",
    "else:\n",
    "    base_FP = '/data'\n",
    "    cpuserver_data_FP = '/data'\n",
    "sys.path.append(base_FP + '/python_modules')\n",
    "\n",
    "import HydroAI.Plot as hPlot\n",
    "import HydroAI.Grid as hGrid\n",
    "import HydroAI.Data as hData\n",
    "import HydroAI.LULC as hLULC\n",
    "importlib.reload(hGrid);\n",
    "importlib.reload(hPlot);\n",
    "importlib.reload(hData);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a100c04-1ad9-48be-8013-cd316cef7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = base_dir = cpuserver_data_FP+\"/CYGNSS/L1_V21\"\n",
    "nc_file_list = hData.get_file_list(base_dir, 'nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360f1376-b547-492f-a119-5cd676c39c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0/406\n",
      "Processing row 100/406\n",
      "Processing row 200/406\n",
      "Processing row 300/406\n",
      "Processing row 400/406\n"
     ]
    }
   ],
   "source": [
    "#ref_lon, ref_lat = hSMAP.get_e2grid(cpuserver_data_FP, 'SPL3SMP.006')\n",
    "ref_lon, ref_lat = hGrid.generate_lon_lat_e2grid('36km')\n",
    "#ref_lon, ref_lat = hGrid.generate_lon_lat_eqdgrid(0.25)\n",
    "\n",
    "data_count = np.zeros_like(ref_lat, dtype=int)\n",
    "# Flatten the reference arrays and stack them as [latitude, longitude]\n",
    "ref_points = np.column_stack((ref_lat.flatten(), ref_lon.flatten()))\n",
    "tree = cKDTree(ref_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06046aa-1e5c-4c9c-b2a8-73db54dc8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays for accumulating sums and counts\n",
    "angle_sum = np.zeros_like(ref_lat, dtype=float)  # Sum values for each grid cell\n",
    "angle_sum_sq = np.zeros_like(ref_lat, dtype=float)  # Sum values for each grid cell\n",
    "data_count = np.zeros_like(ref_lat, dtype=int)  # Count of measurements for each grid cell\n",
    "\n",
    "for i, file_name in tqdm(enumerate(nc_file_list[:10]), total=len(nc_file_list[:10]), desc=\"Processing Files\"):\n",
    "    dataset = nc.Dataset(file_name)\n",
    "    sp_lat = dataset.variables['sp_lat'][:].flatten().compressed()\n",
    "    sp_lon = dataset.variables['sp_lon'][:].flatten().compressed() - 180\n",
    "    sp_inc_angle = dataset.variables['sp_inc_angle'][:].flatten().compressed()\n",
    "    sat_points = np.column_stack((sp_lat, sp_lon))\n",
    "    _, indices = tree.query(sat_points)\n",
    "    \n",
    "    rows, cols = np.unravel_index(indices, ref_lat.shape)\n",
    "\n",
    "    # Update sums, sum of squares, and counts for each point\n",
    "    for row, col, angle in zip(rows, cols, sp_inc_angle):\n",
    "        #row = 66\n",
    "        angle_sum[row, col] += angle\n",
    "        angle_sum_sq[row, col] += angle ** 2\n",
    "        data_count[row, col] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c8c9d3-1cb4-4d45-bc20-aaf12b50f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3km\n",
      "Processing row 0/4872\n",
      "Processing row 100/4872\n",
      "Processing row 200/4872\n",
      "Processing row 300/4872\n",
      "Processing row 400/4872\n",
      "Processing row 500/4872\n",
      "Processing row 600/4872\n",
      "Processing row 700/4872\n",
      "Processing row 800/4872\n",
      "Processing row 900/4872\n",
      "Processing row 1000/4872\n",
      "Processing row 1100/4872\n",
      "Processing row 1200/4872\n",
      "Processing row 1300/4872\n",
      "Processing row 1400/4872\n",
      "Processing row 1500/4872\n",
      "Processing row 1600/4872\n",
      "Processing row 1700/4872\n",
      "Processing row 1800/4872\n",
      "Processing row 1900/4872\n",
      "Processing row 2000/4872\n",
      "Processing row 2100/4872\n",
      "Processing row 2200/4872\n",
      "Processing row 2300/4872\n",
      "Processing row 2400/4872\n",
      "Processing row 2500/4872\n",
      "Processing row 2600/4872\n",
      "Processing row 2700/4872\n",
      "Processing row 2800/4872\n",
      "Processing row 2900/4872\n",
      "Processing row 3000/4872\n",
      "Processing row 3100/4872\n",
      "Processing row 3200/4872\n",
      "Processing row 3300/4872\n",
      "Processing row 3400/4872\n",
      "Processing row 3500/4872\n",
      "Processing row 3600/4872\n",
      "Processing row 3700/4872\n",
      "Processing row 3800/4872\n",
      "Processing row 3900/4872\n",
      "Processing row 4000/4872\n",
      "Processing row 4100/4872\n",
      "Processing row 4200/4872\n",
      "Processing row 4300/4872\n",
      "Processing row 4400/4872\n",
      "Processing row 4500/4872\n",
      "Processing row 4600/4872\n",
      "Processing row 4700/4872\n",
      "Processing row 4800/4872\n",
      "9km\n",
      "Processing row 0/1624\n",
      "Processing row 100/1624\n",
      "Processing row 200/1624\n",
      "Processing row 300/1624\n",
      "Processing row 400/1624\n",
      "Processing row 500/1624\n",
      "Processing row 600/1624\n",
      "Processing row 700/1624\n",
      "Processing row 800/1624\n",
      "Processing row 900/1624\n",
      "Processing row 1000/1624\n",
      "Processing row 1100/1624\n",
      "Processing row 1200/1624\n",
      "Processing row 1300/1624\n",
      "Processing row 1400/1624\n",
      "Processing row 1500/1624\n",
      "Processing row 1600/1624\n",
      "25km\n",
      "Processing row 0/584\n",
      "Processing row 100/584\n",
      "Processing row 200/584\n",
      "Processing row 300/584\n",
      "Processing row 400/584\n",
      "Processing row 500/584\n",
      "36km\n",
      "Processing row 0/406\n",
      "Processing row 100/406\n",
      "Processing row 200/406\n",
      "Processing row 300/406\n",
      "Processing row 400/406\n"
     ]
    }
   ],
   "source": [
    "# Define the list of resolution values\n",
    "resol_values = [\"3km\", \"9km\", \"25km\", \"36km\"]\n",
    "\n",
    "# Dictionary to store data for each resolution\n",
    "data_dict = {}\n",
    "\n",
    "# Iterate over each resolution value\n",
    "for resol in resol_values:\n",
    "    print(resol)\n",
    "    # Get file paths\n",
    "    data_counts_file = hData.get_file_list(cpuserver_data_FP+'/CYGNSS/data_counts_csv', 'csv', filter_strs=[\"data_count_\"+resol+\".csv\"])[0]\n",
    "    angle_sum_file = hData.get_file_list(cpuserver_data_FP+'/CYGNSS/data_counts_csv', 'csv', filter_strs=[\"angle_sum_\"+resol+\".csv\"])[0]\n",
    "    angle_sum_sq_file = hData.get_file_list(cpuserver_data_FP+'/CYGNSS/data_counts_csv', 'csv', filter_strs=[\"angle_sum_sq_\"+resol+\".csv\"])[0]\n",
    "\n",
    "    # Read data from files\n",
    "    data_counts = pd.read_csv(data_counts_file, header=None).values\n",
    "    angle_sum = pd.read_csv(angle_sum_file, header=None).values\n",
    "    angle_sum_sq = pd.read_csv(angle_sum_sq_file, header=None).values\n",
    "\n",
    "    # Generate lon and lat\n",
    "    lon, lat = hGrid.generate_lon_lat_e2grid(resol)\n",
    "\n",
    "    # Create dictionary to store data\n",
    "    data_dict[resol] = {\n",
    "        \"data_counts\": data_counts,\n",
    "        \"angle_sum\": angle_sum,\n",
    "        \"angle_sum_sq\": angle_sum_sq,\n",
    "        \"lon\": lon,\n",
    "        \"lat\": lat\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf36ea-bab4-4b46-8927-726052193787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Accessing data_dict for resolution \"36km\"\n",
    "counts_36 = data_dict[\"36km\"][\"data_counts\"]\n",
    "sum_36 = data_dict[\"36km\"][\"angle_sum\"]\n",
    "sum_sq_36 = data_dict[\"36km\"][\"angle_sum_sq\"]\n",
    "lat_36 = data_dict[\"36km\"][\"lat\"]\n",
    "lon_36 = data_dict[\"36km\"][\"lon\"]\n",
    "\n",
    "counts_25 = data_dict[\"25km\"][\"data_counts\"]\n",
    "sum_25 = data_dict[\"25km\"][\"angle_sum\"]\n",
    "sum_sq_25 = data_dict[\"25km\"][\"angle_sum_sq\"]\n",
    "lat_25 = data_dict[\"25km\"][\"lat\"]\n",
    "lon_25 = data_dict[\"25km\"][\"lon\"]\n",
    "\n",
    "counts_9 = data_dict[\"9km\"][\"data_counts\"]\n",
    "sum_9 = data_dict[\"9km\"][\"angle_sum\"]\n",
    "sum_sq_9 = data_dict[\"9km\"][\"angle_sum_sq\"]\n",
    "lat_9 = data_dict[\"9km\"][\"lat\"]\n",
    "lon_9 = data_dict[\"9km\"][\"lon\"]\n",
    "\n",
    "counts_3 = data_dict[\"3km\"][\"data_counts\"]\n",
    "sum_3 = data_dict[\"3km\"][\"angle_sum\"]\n",
    "sum_sq_3 = data_dict[\"3km\"][\"angle_sum_sq\"]\n",
    "lat_3 = data_dict[\"3km\"][\"lat\"]\n",
    "lon_3 = data_dict[\"3km\"][\"lon\"]\n",
    "\n",
    "FP = cpuserver_data_FP + '/LULC/MCD12C1/'\n",
    "file_list =  hData.get_file_list(FP, 'hdf')\n",
    "input_file = file_list[-1]\n",
    "MCD12C1_t1 = hData.read_hdf4_variable(input_file, 'Majority_Land_Cover_Type_1')\n",
    "lon_eqd_5km, lat_eqd_5km = hGrid.generate_lon_lat_eqdgrid(0.05)\n",
    "\n",
    "LULC_36 = hData.Resampling(lat_36, lon_36, lat_eqd_5km, lon_eqd_5km, MCD12C1_t1, sampling_method='nearest', agg_method='mode', mag_factor=3)\n",
    "LULC_25 = hData.Resampling(lat_25, lon_25, lat_eqd_5km, lon_eqd_5km, MCD12C1_t1, sampling_method='nearest', agg_method='mode', mag_factor=3)\n",
    "LULC_9  = hData.Resampling(lat_9,  lon_9,  lat_eqd_5km, lon_eqd_5km, MCD12C1_t1, sampling_method='nearest', agg_method='mode', mag_factor=3)\n",
    "LULC_3  = hData.Resampling(lat_3,  lon_3,  lat_eqd_5km, lon_eqd_5km, MCD12C1_t1, sampling_method='nearest', agg_method='mode', mag_factor=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48c055-fdb3-46cd-9b0c-b8004e2d7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_plot(data_counts, angle_sum, angle_sum_sq, lon, lat, LULC, resol):\n",
    "    \"\"\"\n",
    "    Calculate the inclination angle and standard deviation, then plot the data on a map.\n",
    "\n",
    "    Args:\n",
    "    - data_counts: Data counts.\n",
    "    - angle_sum: Angle sum.\n",
    "    - angle_sum_sq: Angle sum squared.\n",
    "    - lon: Longitudes.\n",
    "    - lat: Latitudes.\n",
    "    - LULC: Land use land cover data.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Calculate inclination angle average\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        inc_avg = angle_sum / data_counts\n",
    "        inc_std = angle_sum_sq / data_counts - (angle_sum / data_counts) ** 2\n",
    "        inc_std[data_counts == 0] = np.nan  # Set where denominator is zero to NaN\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    val = data_counts.copy()\n",
    "    val[LULC == 0] = np.nan\n",
    "\n",
    "    # Plot the data on a map\n",
    "    importlib.reload(hPlot)\n",
    "    if resol == '36':\n",
    "        max_val = 80000\n",
    "    elif resol == '25':\n",
    "        max_val = 60001\n",
    "    elif resol == '9':\n",
    "        max_val = 60000\n",
    "    elif resol == '3':\n",
    "        max_val = 2000\n",
    "    \n",
    "    hPlot.plot_map(lon, lat, val, 0, max_val, plot_title='EASE2 '+resol+'-km (2017.3 - 2024.3)',\n",
    "                   label_title='Number of Data', cmap='jet_r', projection='Mollweide')\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have the necessary inputs defined\n",
    "calculate_and_plot(counts_36, sum_36, sum_sq_36, lon_36, lat_36, LULC_36, '36')\n",
    "calculate_and_plot(counts_25, sum_25, sum_sq_25, lon_25, lat_25, LULC_25, '25')\n",
    "calculate_and_plot(counts_9, sum_9, sum_sq_9, lon_9, lat_9, LULC_9, '9')\n",
    "calculate_and_plot(counts_3, sum_3, sum_sq_3, lon_3, lat_3, LULC_3, '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d005a-5083-43fc-a88a-b0972a332280",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(hPlot)\n",
    "hPlot.plot_LULC_map_MCD12C1(lon_3, lat_3, LULC_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a45a4b-29da-4f1d-ab3a-2d09591d4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_save_dir = cpuserver_data_FP + '/extracted_nc'\n",
    "mission_product = 'SPL3SMP.008'\n",
    "\n",
    "start_year = 2015\n",
    "end_year = 2023\n",
    "\n",
    "SM = np.array([])\n",
    "VWC = np.array([])\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    # Define the file name based on the year and path\n",
    "    path = 'am'  # Assuming 'am' or 'pm'; adjust logic if it varies yearly\n",
    "    nc_file = f\"{nc_save_dir}/{mission_product}_{year}_{path}.nc\"\n",
    "    \n",
    "    # Open the NetCDF file\n",
    "    nc_data = nc.Dataset(nc_file)\n",
    "\n",
    "    # Load data based on the time of day ('am' or 'pm')\n",
    "    if path == 'pm':\n",
    "        SMAP_SM = nc_data.variables[f'soil_moisture_{path}'][:].data\n",
    "        SMAP_VWC = nc_data.variables[f'vegetation_water_content_{path}'][:].data\n",
    "    else:\n",
    "        SMAP_SM = nc_data.variables['soil_moisture'][:].data\n",
    "        SMAP_VWC = nc_data.variables['vegetation_water_content'][:].data\n",
    "    \n",
    "    # Stack the arrays from each year\n",
    "    if SM.size == 0:  # If the SM array is empty, initialize it\n",
    "        SM = SMAP_SM\n",
    "        VWC = SMAP_VWC\n",
    "    else:  # Otherwise, stack the new data onto the existing array\n",
    "        SM = np.dstack((SM, SMAP_SM))\n",
    "        VWC = np.dstack((VWC, SMAP_VWC))\n",
    "        \n",
    "    if year == start_year:\n",
    "        SMAP_lat = nc_data.variables['latitude'][:].data\n",
    "        SMAP_lon = nc_data.variables['longitude'][:].data\n",
    "    \n",
    "    # Close the NetCDF file\n",
    "    nc_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df76275-f654-4f6a-84e7-16cbdadab875",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_VWC = np.nanmean(VWC,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09e7e9-e806-4600-9970-f3ae3405b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "hPlot.plot_map(SMAP_lon, SMAP_lat, avg_VWC, 0, 20, plot_title='SMAP (SPL3SMP) Vegetation Water Content (2015.4 - 2023.12)', label_title='Vegetation Water Content (kg/m$^{-2}$)', cmap='Greens', projection='Mollweide')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
