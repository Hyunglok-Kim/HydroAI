{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348dccc5-89c1-4afa-a085-cb11a62f3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997ee13e-6c7c-4b14-be7e-e45d1491f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(9, 1)  # 3*3 = 9\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")  # Debugging statement\n",
    "        x = self.conv_layers(x)\n",
    "        print(f\"Shape after conv layers: {x.shape}\")  # Debugging statement\n",
    "        x = self.flatten(x)\n",
    "        print(f\"Shape after flattening: {x.shape}\")  # Debugging statement\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        print(f\"Output shape: {x.shape}\")  # Debugging statement\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6677bc64-c5ad-4b7f-a92a-cb00e3e18957",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [1/10], Batch [0/8], D Loss: 0.7299745082855225, G Loss: 0.9955025911331177\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [2/10], Batch [0/8], D Loss: 0.43345269560813904, G Loss: 1.1180574893951416\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [3/10], Batch [0/8], D Loss: 0.352302610874176, G Loss: 1.4126077890396118\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [4/10], Batch [0/8], D Loss: 0.22406062483787537, G Loss: 2.0116302967071533\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [5/10], Batch [0/8], D Loss: 0.19324851036071777, G Loss: 2.1632494926452637\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [6/10], Batch [0/8], D Loss: 0.10409323871135712, G Loss: 2.7074358463287354\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [7/10], Batch [0/8], D Loss: 0.07198531180620193, G Loss: 3.126929759979248\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [8/10], Batch [0/8], D Loss: 0.05719791352748871, G Loss: 3.2776942253112793\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [9/10], Batch [0/8], D Loss: 0.04257553815841675, G Loss: 3.7863330841064453\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Epoch [10/10], Batch [0/8], D Loss: 0.05766179412603378, G Loss: 3.5492935180664062\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([128, 9])\n",
      "Output shape: torch.Size([128, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n",
      "Input shape: torch.Size([104, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([104, 1, 3, 3])\n",
      "Shape after flattening: torch.Size([104, 9])\n",
      "Output shape: torch.Size([104, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load MNIST data and use a smaller subset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Use a smaller subset of the dataset\n",
    "subset_indices = list(range(1000))  # Use first 1000 samples\n",
    "train_subset = Subset(train_data, subset_indices)\n",
    "test_subset = Subset(test_data, subset_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Create masks\n",
    "def create_mask(images):\n",
    "    masked_images = images.clone()\n",
    "    mask = torch.ones_like(images)\n",
    "    for img in masked_images:\n",
    "        x = np.random.randint(0, img.shape[1] // 2)\n",
    "        y = np.random.randint(0, img.shape[2] // 2)\n",
    "        img[:, x:x+14, y:y+14] = 0\n",
    "        mask[:, x:x+14, y:y+14] = 0\n",
    "    return masked_images, mask\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(3 * 3, 1)  # 3*3 = 9\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")  # Debugging statement\n",
    "        x = self.conv_layers(x)\n",
    "        print(f\"Shape after conv layers: {x.shape}\")  # Debugging statement\n",
    "        x = self.flatten(x)\n",
    "        print(f\"Shape after flattening: {x.shape}\")  # Debugging statement\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        print(f\"Output shape: {x.shape}\")  # Debugging statement\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training\n",
    "epochs = 10  # Reduce the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real_images, _) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        masked_images, masks = create_mask(real_images)\n",
    "        masked_images = masked_images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.ones(real_images.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(real_images.size(0), 1).to(device)\n",
    "        \n",
    "        real_input = torch.cat((real_images, masked_images), 1)\n",
    "        real_output = discriminator(real_input)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "\n",
    "        fake_images = generator(torch.cat((masked_images, masks), 1))\n",
    "        fake_input = torch.cat((fake_images, masked_images), 1)\n",
    "        fake_output = discriminator(fake_input.detach())\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_output = discriminator(fake_input)\n",
    "        g_loss = criterion(fake_output, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:  # Reduce print frequency\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae061d90-5c7d-428e-95c5-ac5c23dc8244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_results(masked_imgs, gen_imgs, orig_imgs, num_images=5):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        # Original images\n",
    "        ax = plt.subplot(3, num_images, i + 1)\n",
    "        plt.imshow(orig_imgs[i].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Masked images\n",
    "        ax = plt.subplot(3, num_images, i + 1 + num_images)\n",
    "        plt.imshow(masked_imgs[i].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Generated images\n",
    "        ax = plt.subplot(3, num_images, i + 1 + num_images * 2)\n",
    "        plt.imshow(gen_imgs[i].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate some images to visualize\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "masked_test_images, test_masks = create_mask(test_images)\n",
    "generated_images = generator(torch.cat((masked_test_images, test_masks), 1))\n",
    "\n",
    "plot_results(masked_test_images, generated_images, test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90127545-d99c-4280-849d-0137fa68426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 2, 28, 28])\n",
      "Shape after conv layers: torch.Size([128, 1, 1, 1])\n",
      "Shape after flattening: torch.Size([128, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x1 and 9x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 135\u001b[0m\n\u001b[1;32m    132\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(real_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    134\u001b[0m real_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((real_images, masked_images), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m d_loss_real \u001b[38;5;241m=\u001b[39m criterion(real_output, real_labels)\n\u001b[1;32m    138\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m generator(torch\u001b[38;5;241m.\u001b[39mcat((masked_images, masks), \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hydroai/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hydroai/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 100\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape after flattening: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging statement\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(x)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Debugging statement\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hydroai/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hydroai/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hydroai/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x1 and 9x1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load MNIST data and use a slightly larger subset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Use a larger subset of the dataset\n",
    "subset_indices = list(range(5000))  # Use first 5000 samples\n",
    "train_subset = Subset(train_data, subset_indices)\n",
    "test_subset = Subset(test_data, subset_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Create masks\n",
    "def create_mask(images):\n",
    "    masked_images = images.clone()\n",
    "    mask = torch.ones_like(images)\n",
    "    for img in masked_images:\n",
    "        x = np.random.randint(0, img.shape[1] // 2)\n",
    "        y = np.random.randint(0, img.shape[2] // 2)\n",
    "        img[:, x:x+14, y:y+14] = 0\n",
    "        mask[:, x:x+14, y:y+14] = 0\n",
    "    return masked_images, mask\n",
    "\n",
    "# Define the Generator with a slightly improved architecture\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the Discriminator with a slightly improved architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(3 * 3, 1)  # 3*3 = 9\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")  # Debugging statement\n",
    "        x = self.conv_layers(x)\n",
    "        print(f\"Shape after conv layers: {x.shape}\")  # Debugging statement\n",
    "        x = self.flatten(x)\n",
    "        print(f\"Shape after flattening: {x.shape}\")  # Debugging statement\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        print(f\"Output shape: {x.shape}\")  # Debugging statement\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=1, gamma=0.9)\n",
    "scheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=1, gamma=0.9)\n",
    "\n",
    "# Training\n",
    "epochs = 10  # Increase the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real_images, _) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        masked_images, masks = create_mask(real_images)\n",
    "        masked_images = masked_images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_labels = torch.ones(real_images.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(real_images.size(0), 1).to(device)\n",
    "        \n",
    "        real_input = torch.cat((real_images, masked_images), 1)\n",
    "        real_output = discriminator(real_input)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "\n",
    "        fake_images = generator(torch.cat((masked_images, masks), 1))\n",
    "        fake_input = torch.cat((fake_images, masked_images), 1)\n",
    "        fake_output = discriminator(fake_input.detach())\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "\n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_output = discriminator(fake_input)\n",
    "        g_loss = criterion(fake_output, real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:  # Reduce print frequency\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "\n",
    "# Visualize Results\n",
    "def plot_results(masked_imgs, gen_imgs, orig_imgs, num_images=5):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        # Original images\n",
    "        ax = plt.subplot(3, num_images, i + 1)\n",
    "        plt.imshow(orig_imgs[i].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Masked images\n",
    "        ax = plt.subplot(3, num_images, i + 1 + num_images)\n",
    "        plt.imshow(masked_imgs[i].cpu().numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Generated images\n",
    "        ax = plt.subplot(3, num_images, i + 1 + num_images * 2)\n",
    "        plt.imshow(gen_imgs[i].cpu().detach().numpy().reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate some images to visualize\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "masked_test_images, test_masks = create_mask(test_images)\n",
    "generated_images = generator(torch.cat((masked_test_images, test_masks), 1))\n",
    "\n",
    "plot_results(masked_test_images, generated_images, test_images)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
