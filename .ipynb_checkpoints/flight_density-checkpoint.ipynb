{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6009d631-9fa5-4f26-9901-03147a3c2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# tar/zip related libs\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "from ease_grid import EASE2_grid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc15697-b8d3-479f-9a1f-9f5bdb5f5656",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'date' for 'datetime.datetime' objects doesn't apply to a 'int' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to download \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Define the start and end dates\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2020\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m end_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdate(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Create a folder to store the downloaded files\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'date' for 'datetime.datetime' objects doesn't apply to a 'int' object"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def generate_dates(start_date, end_date):\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        yield current_date\n",
    "        current_date += datetime.timedelta(weeks=1)\n",
    "\n",
    "def download_file(url, folder):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    path = os.path.join(folder, local_filename)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                f.write(chunk)\n",
    "    return local_filename\n",
    "\n",
    "def download_files(urls, folder):\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(download_file, url, folder): url for url in urls}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime.date(2020, 5, 25)\n",
    "end_date = datetime.date(2020, 6, 2)\n",
    "\n",
    "# Create a folder to store the downloaded files\n",
    "folder = '/data/opensky_data'\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Prepare a list of URLs to download\n",
    "urls_to_download = []\n",
    "for date in generate_dates(start_date, end_date):\n",
    "    for hour in range(24):\n",
    "        url = f\"https://opensky-network.org/datasets/states/{date}/{str(hour).zfill(2)}/states_{date}-{str(hour).zfill(2)}.csv.tar\"\n",
    "        urls_to_download.append(url)\n",
    "\n",
    "# Download files in parallel\n",
    "download_files(urls_to_download, folder)\n",
    "\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c89767-5458-4660-a993-cbeb9bd71a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tar_files(tar_directory, gz_directory):\n",
    "    # Create the gz directory if it doesn't exist\n",
    "    if not os.path.exists(gz_directory):\n",
    "        os.makedirs(gz_directory)\n",
    "\n",
    "    # Loop through all files in the tar_directory\n",
    "    for filename in os.listdir(tar_directory):\n",
    "        if filename.endswith(\".tar\"):\n",
    "            tar_path = os.path.join(tar_directory, filename)\n",
    "            # Extract the .tar file\n",
    "            with tarfile.open(tar_path) as tar:\n",
    "                tar.extractall(path=tar_directory)\n",
    "\n",
    "def decompress_gz_files(tar_directory, gz_directory):\n",
    "    # Loop through all files in the tar_directory\n",
    "    for filename in os.listdir(tar_directory):\n",
    "        if filename.endswith(\".gz\"):\n",
    "            gz_path = os.path.join(tar_directory, filename)\n",
    "            # Decompress .gz file\n",
    "            with gzip.open(gz_path, 'rb') as f_in:\n",
    "                with open(os.path.join(gz_directory, filename[:-3]), 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            # Optional: Remove the .gz file after decompression\n",
    "            os.remove(gz_path)\n",
    "\n",
    "# Define your directories\n",
    "tar_directory = '/Volumes/data/opensky_data/'\n",
    "gz_directory = os.path.join(tar_directory, \"gz\")\n",
    "\n",
    "# Extract tar files and then decompress gz files\n",
    "extract_tar_files(tar_directory, gz_directory)\n",
    "decompress_gz_files(tar_directory, gz_directory)\n",
    "\n",
    "print(\"Extraction and decompression complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6d4594-1e9f-404d-8982-7394f23882f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path to the \"gz\" folder containing CSV files\n",
    "gz_folder_path = '/Users/hyunglokkim/cpuserver_data/gz'\n",
    "#gz_folder_path = '/data/gz'\n",
    "\n",
    "# Initialize lists to store latitude, longitude, and time values\n",
    "latitudes = []\n",
    "longitudes = []\n",
    "times = []\n",
    "\n",
    "# Get the list of all files in the directory\n",
    "files = [f for f in os.listdir(gz_folder_path) if f.endswith('.csv')]\n",
    "total_files = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e196ac-2407-469d-bed2-cfb21e6453d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_egrid(resolution):\n",
    "    if resolution == 'EASE2_3km':\n",
    "        egrid = 3000\n",
    "    elif resolution == 'EASE2_9km':\n",
    "        egrid = 9000\n",
    "    elif resolution == 'EASE2_12.5km':\n",
    "        egrid = 12500\n",
    "    elif resolution == 'EASE2_25km':\n",
    "        egrid = 25000\n",
    "    elif resolution == 'EASE2_36km':\n",
    "        egrid = 36000\n",
    "    else:\n",
    "        egrid = None  # or some default value, or raise an error\n",
    "    return egrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51961a6-1146-453c-8e40-eb9b123cc8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (112, 75, 18336)\n",
      "Time range: 2020-05-25 00:00:00 to 2022-06-27 23:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   2%|â–Ž                   | 38/2462 [08:39<12:01:02, 17.85s/it]"
     ]
    }
   ],
   "source": [
    "# Define the region of interest (either \"Korea\" or \"globe\")\n",
    "region = \"Korea\"  # Change this to \"globe\" to use all lat/lon data\n",
    "# Select the resolution\n",
    "resolution = 'EASE2_9km'  # Change this to select a different resolution\n",
    "\n",
    "egrid =EASE2_grid(set_egrid(resolution))\n",
    "\n",
    "# Korean Peninsula coordinates (if needed)\n",
    "lat_min, lat_max = 33, 43\n",
    "lon_min, lon_max = 124, 131\n",
    "\n",
    "lons_2d, lats_2d = np.meshgrid(egrid.londim, egrid.latdim)\n",
    "\n",
    "# If the region is \"Korea\", calculate the indices for the area of interest\n",
    "if region == \"Korea\":\n",
    "    lat_indices = np.where((lats_2d >= lat_min) & (lats_2d <= lat_max))\n",
    "    lon_indices = np.where((lons_2d >= lon_min) & (lons_2d <= lon_max))\n",
    "\n",
    "    # Use the indices to create cropped latitude and longitude arrays\n",
    "    cropped_lats_2d = lats_2d[lat_indices[0][0]:lat_indices[0][-1] + 1, lon_indices[1][0]:lon_indices[1][-1] + 1]\n",
    "    cropped_lons_2d = lons_2d[lat_indices[0][0]:lat_indices[0][-1] + 1, lon_indices[1][0]:lon_indices[1][-1] + 1]\n",
    "\n",
    "# Path to the \"gz\" folder containing CSV files\n",
    "files = sorted([f for f in os.listdir(gz_folder_path) if f.endswith('.csv')])\n",
    "\n",
    "# Extract timestamps from file names and sort them\n",
    "timestamps = [datetime.strptime(f.split('_')[1].split('.')[0], '%Y-%m-%d-%H') for f in files]\n",
    "timestamps.sort()\n",
    "\n",
    "# Calculate the range of timestamps\n",
    "earliest_timestamp = timestamps[0]\n",
    "latest_timestamp = timestamps[-1]\n",
    "\n",
    "# Calculate the total number of hours between the earliest and latest timestamp\n",
    "total_hours = int((latest_timestamp - earliest_timestamp).total_seconds() / 3600) + 1\n",
    "\n",
    "# 'x' is the total number of unique hourly layers\n",
    "x = total_hours\n",
    "\n",
    "# Create the number_of_flights array with dynamic shape\n",
    "number_of_flights = np.zeros((cropped_lats_2d.shape[0], cropped_lats_2d.shape[1], x), dtype=np.int8)\n",
    "\n",
    "# Print the array shape and the range of dates\n",
    "print(f\"Array shape: {number_of_flights.shape}\")\n",
    "print(f\"Time range: {earliest_timestamp} to {latest_timestamp}\")\n",
    "\n",
    "# Function to find the closest index in the grid for a given latitude or longitude\n",
    "def find_closest_index(value, grid):\n",
    "    return np.abs(grid - value).argmin()\n",
    "    \n",
    "# Turn off DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Initialize the progress bar\n",
    "pbar = tqdm(total=len(files), desc=\"Processing files\")\n",
    "\n",
    "for file in files[:1000]:\n",
    "    file_path = os.path.join(gz_folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Filter out rows where latitude or longitude is outside the specified range\n",
    "    if region == \"Korea\":\n",
    "        df = df[(df['lat'] >= lat_min) & (df['lat'] <= lat_max) & (df['lon'] >= lon_min) & (df['lon'] <= lon_max)]\n",
    "\n",
    "    # Check if the filtered DataFrame is not empty\n",
    "    if not df.empty:\n",
    "        for index, row in df.iterrows():\n",
    "            # Find the closest grid cell in the cropped 2D grid\n",
    "            grid_lat_idx = find_closest_index(row['lat'], cropped_lats_2d[:, 0])\n",
    "            grid_lon_idx = find_closest_index(row['lon'], cropped_lons_2d[0, :])\n",
    "\n",
    "            # Calculate the hourly layer index based on time\n",
    "            time = datetime.utcfromtimestamp(row['time'])\n",
    "            hourly_layer_idx = int((time - earliest_timestamp).total_seconds() / 3600)\n",
    "\n",
    "            # Increment the count in the corresponding cell\n",
    "            number_of_flights[grid_lat_idx, grid_lon_idx, hourly_layer_idx] += 1\n",
    "\n",
    "        # Update the progress bar for each file\n",
    "        pbar.update(1)\n",
    "\n",
    "# Close the progress bar after processing all files\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b57643-7453-4c1c-bf5a-31a83e902f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_flights = np.sum(number_of_flights,2).astype('float64')\n",
    "summed_flights[summed_flights <= 0 ] = np.nan\n",
    "summed_flights[summed_flights > 300] = 300\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define the map projection\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "# Set the extent to the Korean Peninsula\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add country borders and coastlines\n",
    "ax.add_feature(cfeature.BORDERS, edgecolor='black')\n",
    "ax.coastlines()\n",
    "\n",
    "# Add gridlines\n",
    "ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "\n",
    "# Create the mesh plot using cropped_lons_2d and cropped_lats_2d\n",
    "mesh = plt.pcolormesh(cropped_lons_2d, cropped_lats_2d, summed_flights, shading='auto', transform=ccrs.PlateCarree())\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Summed Flights over Korean Peninsula on EASE2 Grid')\n",
    "\n",
    "# Define the position and size of the color bar manually\n",
    "cbar_axes = fig.add_axes([0.31, 0.05, 0.4, 0.02])  # [left, bottom, width, height]\n",
    "\n",
    "# Add the color bar\n",
    "cbar = plt.colorbar(mesh, cax=cbar_axes, orientation='horizontal')\n",
    "\n",
    "# Get the minimum and maximum of summed_flights for the color bar ticks\n",
    "min_val, max_val = np.nanmin(summed_flights), np.nanmax(summed_flights)\n",
    "\n",
    "# Create 5 evenly spaced ticks from min_val to max_val\n",
    "# Ensure that the minimum tick starts from 1\n",
    "ticks = np.linspace(max(min_val, 1), max_val, 5)\n",
    "\n",
    "# Set ticks and tick labels\n",
    "cbar.set_ticks(ticks)\n",
    "cbar.set_ticklabels([f\"{int(tick)}\" for tick in ticks])\n",
    "\n",
    "# Set the color bar label\n",
    "cbar.set_label('Number of Flights')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d34ad-ad7c-4c46-9b43-064fc5b7e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(summed_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9931ff5-d02c-43a2-ae25-9c5d44596b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the dtype and shape according to your data's specification\n",
    "lon_shape = (406, 964) # replace with actual dimensions of longitude data\n",
    "lat_shape = (406, 964) # replace with actual dimensions of latitude data\n",
    "\n",
    "lons = np.fromfile('/data/EASE2/EASE2_M36km.lons.964x406x1.double', dtype=np.double)\n",
    "lats = np.fromfile('/data/EASE2/EASE2_M36km.lats.964x406x1.double', dtype=np.double)\n",
    "\n",
    "lons = lons.reshape(lon_shape)\n",
    "lats = lats.reshape(lat_shape)\n",
    "\n",
    "# Create an array of NaN values\n",
    "data = np.full(lon_shape, np.nan).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e2d89-9943-4b91-b97d-22559b770b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_edges(arr):\n",
    "    # Interpolate between points to approximate the edges\n",
    "    arr_expanded = np.zeros((arr.shape[0]+1, arr.shape[1]+1))\n",
    "    arr_expanded[1:-1, 1:-1] = (arr[:-1, :-1] + arr[1:, :-1] + arr[:-1, 1:] + arr[1:, 1:]) / 4\n",
    "    arr_expanded[0, 1:-1] = (arr[0, :-1] + arr[0, 1:]) / 2\n",
    "    arr_expanded[-1, 1:-1] = (arr[-1, :-1] + arr[-1, 1:]) / 2\n",
    "    arr_expanded[1:-1, 0] = (arr[:-1, 0] + arr[1:, 0]) / 2\n",
    "    arr_expanded[1:-1, -1] = (arr[:-1, -1] + arr[1:, -1]) / 2\n",
    "    arr_expanded[0, 0] = arr[0, 0]\n",
    "    arr_expanded[-1, -1] = arr[-1, -1]\n",
    "    arr_expanded[0, -1] = arr[0, -1]\n",
    "    arr_expanded[-1, 0] = arr[-1, 0]\n",
    "    return arr_expanded\n",
    "\n",
    "lons_edges = interpolate_edges(lons)\n",
    "lats_edges = interpolate_edges(lats)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_extent([124, 132, 33, 43], crs=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False, color='none')\n",
    "\n",
    "# Plotting the approximate grid edges\n",
    "for i in range(lons_edges.shape[0]):\n",
    "    ax.plot(lons_edges[i, :], lats_edges[i, :], color='black', alpha=0.5, linewidth=0.5, transform=ccrs.Geodetic())\n",
    "\n",
    "for i in range(lats_edges.shape[1]):\n",
    "    ax.plot(lons_edges[:, i], lats_edges[:, i], color='black', alpha=0.5, linewidth=0.5, transform=ccrs.Geodetic())\n",
    "\n",
    "plt.title(\"Approximate EASE-Grid Edges Visualization over the Korean Peninsula\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848fd39-f254-4ca0-b145-f4b893fa2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map resolutions to their grid sizes and file names\n",
    "grid_info = {\n",
    "    'EASE2_3km': {'lat_size': 4872, 'lon_size': 11568, 'lon_file': 'EASE2_M03km.lons.11568x4872x1.double', 'lat_file': 'EASE2_M03km.lats.11568x4872x1.double'},\n",
    "    'EASE2_9km': {'lat_size': 1624, 'lon_size': 3856, 'lon_file': 'EASE2_M09km.lons.3856x1624x1.double', 'lat_file': 'EASE2_M09km.lats.3856x1624x1.double'},\n",
    "    'EASE2_12.5km': {'lat_size': 1168, 'lon_size': 2776, 'lon_file': 'EASE2_M12.5km.lons.2776x1168x1.double', 'lat_file': 'EASE2_M12.5km.lats.2776x1168x1.double'},\n",
    "    'EASE2_25km': {'lat_size': 584, 'lon_size': 1388, 'lon_file': 'EASE2_M25km.lons.1388x584x1.double', 'lat_file': 'EASE2_M25km.lats.1388x584x1.double'},\n",
    "    'EASE2_36km': {'lat_size': 406, 'lon_size': 964, 'lon_file': 'EASE2_M36km.lons.964x406x1.double', 'lat_file': 'EASE2_M36km.lats.964x406x1.double'}\n",
    "}j\n",
    "\n",
    "# Set the grid-related variables based on the selected resolution\n",
    "lat_size = grid_info[resolution]['lat_size']\n",
    "lon_size = grid_info[resolution]['lon_size']\n",
    "lon_file = grid_info[resolution]['lon_file']\n",
    "lat_file = grid_info[resolution]['lat_file']\n",
    "\n",
    "# Read EASE2 grid values\n",
    "lons_1d = np.fromfile(f'/data/EASE2/{lon_file}', dtype=np.double)\n",
    "lats_1d = np.fromfile(f'/data/EASE2/{lat_file}', dtype=np.double)\n",
    "\n",
    "# Reshape the 1D arrays to 2D grids\n",
    "lons_2d = lons_1d.reshape((lat_size, lon_size))\n",
    "lats_2d = lats_1d.reshape((lat_size, lon_size))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
