{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de9652-f92c-4d19-a3c8-000d83301163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import platform\n",
    "import importlib\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "import datetime\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    base_FP = '/Users/hyunglokkim/Insync/hkim@geol.sc.edu/Google_Drive'\n",
    "    sys.path.append(base_FP + '/python_modules')\n",
    "    nc_save_dir = '/Users/hyunglokkim/cpuserver_data'\n",
    "else:\n",
    "    base_FP = '/home/h/Insync/hkim@geol.sc.edu/Google Drive'\n",
    "    sys.path.append(base_FP + '/python_modules')\n",
    "    nc_save_dir = '/data'\n",
    "    \n",
    "import hydroAI.SMOS_IC as hSMOS_IC\n",
    "import hydroAI.ASCAT_TUW as hASCAT_TUW\n",
    "import hydroAI.Plot as Plot\n",
    "import hydroAI.Data as Data\n",
    "importlib.reload(hSMOS_IC);\n",
    "importlib.reload(hASCAT_TUW);\n",
    "importlib.reload(Data);\n",
    "import warnings\n",
    "\n",
    "# Ignore runtime warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc0724-42cd-4fa0-8bed-9f9ea6533539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filelist_doy(directory, year):\n",
    "    data = []\n",
    "\n",
    "    # Iterate over the subdirectories within the specified directory\n",
    "    for subdir in os.listdir(directory):\n",
    "        sub_dir_path = os.path.join(directory, subdir)\n",
    "        \n",
    "        # Check if the item is a directory and matches the 'yyyy.mm.dd' format\n",
    "        if os.path.isdir(sub_dir_path) and len(subdir) == 10 and subdir[4] == '.' and subdir[7] == '.':\n",
    "            # Convert the 'yyyy.mm.dd' format to 'yyyy-mm-dd'\n",
    "            date_str = '-'.join(subdir.split('.'))\n",
    "            # Convert the date string to a datetime object\n",
    "            date_obj = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "            # Process only if the year matches the specified year\n",
    "            if date_obj.year == year:\n",
    "                # Search for .h5 files within the subdirectory\n",
    "                h5_files = glob.glob(os.path.join(sub_dir_path, '*.h5'))\n",
    "\n",
    "                # Get the day of the year (DOY) number\n",
    "                doy = date_obj.timetuple().tm_yday\n",
    "\n",
    "                # Append file paths and corresponding DOY to the data list\n",
    "                for file in h5_files:\n",
    "                    data.append((file, doy))\n",
    "\n",
    "    # Sort the data list based on DOY (and date)\n",
    "    data.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Unzip the sorted data into separate lists\n",
    "    file_list, data_doy = zip(*data) if data else ([], [])\n",
    "\n",
    "    return file_list, data_doy\n",
    "\n",
    "def get_e2grid(cpuserver_data_FP, mission_product):\n",
    "    if mission_product.startswith('SPL3SMP.'):\n",
    "        grid_prefix = 'EASE2_M36km'\n",
    "        shape = (964, 406)\n",
    "    elif mission_product.startswith('SPL3SMP_E.'):\n",
    "        grid_prefix = 'EASE2_M09km'\n",
    "        shape = (3856, 1624)\n",
    "    else:\n",
    "        return None, None  # or some default value, or raise an error\n",
    "\n",
    "    # Construct file paths for latitude and longitude\n",
    "    # File names use shape in the order (lat_size, lon_size)\n",
    "    grid_size = f\"{shape[0]}x{shape[1]}x1\"\n",
    "    lats_FP = os.path.join(cpuserver_data_FP, f'grids/{grid_prefix}.lats.{grid_size}.double')\n",
    "    lons_FP = os.path.join(cpuserver_data_FP, f'grids/{grid_prefix}.lons.{grid_size}.double')\n",
    "\n",
    "    # Load and reshape data\n",
    "    # Reshape uses flipped dimensions (lon_size, lat_size)\n",
    "    latitude = np.fromfile(lats_FP, dtype=np.float64).reshape((shape[1], shape[0]))\n",
    "    longitude = np.fromfile(lons_FP, dtype=np.float64).reshape((shape[1], shape[0]))\n",
    "\n",
    "    return longitude, latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046a2b9-4146-44d6-98d5-fad3a4ec6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from ease_grid import EASE2_grid\n",
    "\n",
    "def create_array_from_h5(file_list, data_doy, year, cpuserver_data_FP, mission_product, variable_name, group_name):\n",
    "    x, y = None, None\n",
    "    doy_max = 366 if calendar.isleap(year) else 365\n",
    "\n",
    "    # Initialize data_array with NaNs\n",
    "    data_array = None\n",
    "\n",
    "    # Loop over the file list with a progress bar\n",
    "    for i, h5_file in enumerate(tqdm(file_list, desc=\"Processing files\", unit=\"file\")):\n",
    "        try:\n",
    "            with h5py.File(h5_file, 'r') as hdf5_data:\n",
    "                dataset = hdf5_data[group_name][variable_name]\n",
    "                \n",
    "                if data_array is None:\n",
    "                    # Get the shape of dataset from the first file\n",
    "                    x, y = dataset[:].shape\n",
    "                    data_array = np.full((x, y, doy_max + 1), np.nan)  # Create the array filled with NaN\n",
    "\n",
    "                t_data = dataset[:].astype(np.float64)\n",
    "\n",
    "                # Get attributes and apply them if they exist\n",
    "                fill_value = np.float64(dataset.attrs.get('_FillValue', np.nan))\n",
    "                valid_min = np.float64(dataset.attrs.get('valid_min', -np.inf))\n",
    "                valid_max = np.float64(dataset.attrs.get('valid_max', np.inf))\n",
    "                scale_factor = np.float64(dataset.attrs.get('scale_factor', 1.0))\n",
    "                add_offset = np.float64(dataset.attrs.get('add_offset', 0.0))\n",
    "\n",
    "                # Apply scale and offset if applicable\n",
    "                if 'scale_factor' in dataset.attrs or 'add_offset' in dataset.attrs:\n",
    "                    t_data = t_data * scale_factor + add_offset\n",
    "\n",
    "                # Mask invalid data\n",
    "                t_data = np.where((t_data < valid_min) | (t_data > valid_max) | (t_data == fill_value), np.nan, t_data)\n",
    "\n",
    "                # Get the corresponding doy value from the data_doy list\n",
    "                doy = data_doy[i]\n",
    "\n",
    "                # Assign the data to the array\n",
    "                data_array[:, :, doy] = t_data\n",
    "\n",
    "        except OSError as e:\n",
    "            print(f\"Error processing file {h5_file}: {e}\")\n",
    "            if data_array is not None and x is not None and y is not None:\n",
    "                # Mark the entire day as NaN in case of an error\n",
    "                data_array[:, :, data_doy[i]] = np.nan\n",
    "\n",
    "    # Get EASE2 lat/lon from the get_e2grid function\n",
    "    longitude, latitude = get_e2grid(cpuserver_data_FP, mission_product)\n",
    "    \n",
    "    return data_array, longitude, latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ceb230-e590-4b30-9c08-de6dab3295cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_netcdf_file(nc_file, latitude, longitude, **data_vars):\n",
    "    # Create a new NetCDF file\n",
    "    nc_data = netCDF4.Dataset(nc_file, 'w')\n",
    "\n",
    "    # Define the dimensions\n",
    "    rows, cols = latitude.shape\n",
    "    # Assuming all data variables have the same 'time' dimension size\n",
    "    doy = next(iter(data_vars.values())).shape[2]\n",
    "\n",
    "    # Create dimensions in the NetCDF file\n",
    "    nc_data.createDimension('latitude', rows)\n",
    "    nc_data.createDimension('longitude', cols)\n",
    "    nc_data.createDimension('doy', doy)\n",
    "\n",
    "    # Create latitude and longitude variables\n",
    "    lat_var = nc_data.createVariable('latitude', 'f4', ('latitude', 'longitude'))\n",
    "    lon_var = nc_data.createVariable('longitude', 'f4', ('latitude', 'longitude'))\n",
    "\n",
    "    # Assign data to the latitude and longitude variables\n",
    "    lat_var[:] = latitude\n",
    "    lon_var[:] = longitude\n",
    "\n",
    "    # Create variables and assign data for each item in data_vars\n",
    "    for var_name, var_data in data_vars.items():\n",
    "        # Create variable in NetCDF file\n",
    "        nc_var = nc_data.createVariable(var_name, 'f4', ('latitude', 'longitude', 'doy'))\n",
    "        # Assign data to the variable\n",
    "        nc_var[:] = var_data\n",
    "\n",
    "    # Close the NetCDF file\n",
    "    nc_data.close()\n",
    "\n",
    "    print(f\"NetCDF file {nc_file} created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390e921c-1f33-487a-a213-24ad4e9c63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "cpuserver_data_FP = '/Users/hyunglokkim/cpuserver_data'\n",
    "nc_save_dir = 'extracted_nc'\n",
    "mission = 'SMAP'\n",
    "mission_product = 'SPL3SMP_E.005'\n",
    "year = 2022\n",
    "directory = os.path.join(cpuserver_data_FP, mission, mission_product)\n",
    "file_list, data_doy = extract_filelist_doy(directory, year) \n",
    "longitude, latitude = get_e2grid(cpuserver_data_FP, mission_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074e4b6-eecd-4b28-8272-6df37d2e423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example - create_array_from_h5\n",
    "group_name = 'Soil_Moisture_Retrieval_Data_AM'\n",
    "variable_name = 'soil_moisture'\n",
    "SMAP_SM_AM, longitude, latitude = create_array_from_h5(file_list, data_doy, year, cpuserver_data_FP,mission_product, variable_name, group_name)\n",
    "variable_name = 'retrieval_qual_flag'\n",
    "SMAP_QF_AM, longitude, latitude = create_array_from_h5(file_list, data_doy, year, cpuserver_data_FP,mission_product, variable_name, group_name)\n",
    "variable_name = 'tb_qual_flag_3'\n",
    "SMAP_tb3_flag_AM, longitude, latitude = create_array_from_h5(file_list, data_doy, year, cpuserver_data_FP,mission_product, variable_name, group_name)\n",
    "variable_name = 'tb_qual_flag_4'\n",
    "SMAP_tb4_flag_AM, longitude, latitude = create_array_from_h5(file_list, data_doy, year, cpuserver_data_FP,mission_product, variable_name, group_name)\n",
    "variable_name = 'soil_moisture_error' \n",
    "SMAP_SM_error_AM, longitude, latitude = create_array_from_h5(file_list, data_doy, year, cpuserver_data_FP,mission_product, variable_name, group_name)\n",
    "variable_name = 'vegetation_water_content' \n",
    "SMAP_VWC_AM, longitude, latitude = create_array_from_h5(file_list, data_doy, year, cpuserver_data_FP,mission_product, variable_name, group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dea5f-d32d-4e0b-8b8e-89d409cdd4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example - create_netcdf_file\n",
    "# SMAPL3 AM\n",
    "nc_file_name = os.path.join(cpuserver_data_FP,nc_save_dir)+'/'+mission_product+'_'+str(year)+'_am.nc'\n",
    "\n",
    "create_netcdf_file(\n",
    "    nc_file=nc_file_name, \n",
    "    latitude=latitude, \n",
    "    longitude=longitude, \n",
    "    SMAP_SM_AM=SMAP_SM_AM,\n",
    "    SMAP_QF_AM=SMAP_QF_AM,\n",
    "    SMAP_tb3_flag_AM=SMAP_tb3_flag_AM,\n",
    "    SMAP_tb4_flag_AM=SMAP_tb4_flag_AM,\n",
    "    SMAP_SM_error_AM=SMAP_SM_error_AM,\n",
    "    SMAP_VWC_AM=SMAP_VWC_AM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592c187-0a44-4204-b9e4-11c78b9c21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through everything from 2015 to 2023 for AM/PM data\n",
    "cpuserver_data_FP = '/Users/hyunglokkim/cpuserver_data'\n",
    "mission = 'SMAP'\n",
    "mission_product = 'SPL3SMP.008'\n",
    "nc_save_dir = 'extracted_nc'\n",
    "\n",
    "# Ensure the directory for saving NetCDF files exists\n",
    "os.makedirs(os.path.join(cpuserver_data_FP, nc_save_dir), exist_ok=True)\n",
    "\n",
    "# Variables to process\n",
    "variables = [\n",
    "    'soil_moisture',\n",
    "    'retrieval_qual_flag',\n",
    "    'tb_qual_flag_3',\n",
    "    'tb_qual_flag_4',\n",
    "    'soil_moisture_error',\n",
    "    'vegetation_water_content'\n",
    "]\n",
    "\n",
    "for year in range(2015, 2024):\n",
    "    directory = os.path.join(cpuserver_data_FP, mission, mission_product)\n",
    "    file_list, data_doy = extract_filelist_doy(directory, year)\n",
    "    longitude, latitude = get_e2grid(cpuserver_data_FP, mission_product)\n",
    "\n",
    "    # Loop for AM and PM\n",
    "    for suffix in ['AM', 'PM']:\n",
    "        group_name = f'Soil_Moisture_Retrieval_Data_{suffix}'\n",
    "\n",
    "        # Collect data for each variable\n",
    "        data_vars = {}\n",
    "        for var in variables:\n",
    "            variable_name = f\"{var}_{suffix.lower()}\" if suffix == 'PM' else var\n",
    "            data, _, _ = create_array_from_h5(file_list, data_doy, year, cpuserver_data_FP, mission_product, variable_name, group_name)\n",
    "            data_vars[variable_name] = data\n",
    "        \n",
    "        # Save to NetCDF file\n",
    "        nc_file_name = os.path.join(cpuserver_data_FP, nc_save_dir, f'{mission_product}_{year}_{suffix.lower()}.nc')\n",
    "        create_netcdf_file(nc_file=nc_file_name, latitude=latitude, longitude=longitude, **data_vars)\n",
    "        print(f\"Data for {year} {suffix} saved to {nc_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e36ff6-d27e-4b49-b7cb-b8f9e41efbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "nanmean_SMAP_SM_AM = np.nanmean(SMAP_SM_AM, axis=2)\n",
    "target = nanmean_SMAP_SM_AM\n",
    "Plot.plot_global_map(longitude, latitude, target, 'SM', np.nanmin(target), np.nanmax(target), 'jet_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81757802-b121-4f0f-aaf3-0ae5549ab269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_flags(value, flag_value):\n",
    "    return (value & flag_value) == flag_value\n",
    "\n",
    "# Example usage:\n",
    "value = 65534  # This is the value you got from the 'tb_qual_flag_4' dataset\n",
    "flag_masks = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 4096, 16384, 32768]\n",
    "\n",
    "flag_meanings = [\n",
    "    \"4th_Stokes_quality\",\n",
    "    \"4th_Stokes_range\",\n",
    "    \"4th_Stokes_RFI_detection\",\n",
    "    \"4th_Stokes_RFI_correction\",\n",
    "    \"4th_Stokes_NEDT\",\n",
    "    \"4th_Stokes_direct_sun_correction\",\n",
    "    \"4th_Stokes_reflected_sun_correction\",\n",
    "    \"4th_Stokes_reflected_moon_correction\",\n",
    "    \"4th_Stokes_direct_galaxy_correction\",\n",
    "    \"4th_Stokes_reflected_galaxy_correction\",\n",
    "    \"4th_Stokes_atmosphere_correction\",\n",
    "    \"4th_Stokes_null_value_bit\",\n",
    "    \"4th_Stokes_RFI_check\",\n",
    "    \"4th_Stokes_RFI_clean\"\n",
    "]\n",
    "\n",
    "for flag, meaning in zip(flag_masks, flag_meanings):\n",
    "    if check_flags(value, flag):\n",
    "        print(f\"{meaning} flag is set\")\n",
    "    else:\n",
    "        print(f\"{meaning} flag is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697004c-c4c6-4be0-8671-9aa092dc4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Constants\n",
    "seoul_coords = (37.5665, 126.9780)\n",
    "rfi_flag_mask = 4  # Replace with the actual mask value for RFI detection\n",
    "nan_placeholder = -1  # An integer placeholder for NaN values\n",
    "\n",
    "# Function to find the closest index\n",
    "def find_closest_index(latitudes, longitudes, point):\n",
    "    lat_lon = np.c_[latitudes.ravel(), longitudes.ravel()]\n",
    "    tree = cKDTree(lat_lon)\n",
    "    dist, idx = tree.query(point, k=1)\n",
    "    return np.unravel_index(idx, latitudes.shape)\n",
    "\n",
    "# Find the closest pixel to Seoul\n",
    "closest_pixel_index = find_closest_index(latitude, longitude, seoul_coords)\n",
    "\n",
    "# Extract the time series of SMAP_tb4_flag values for the closest pixel\n",
    "tb4_flag_timeseries = SMAP_tb4_flag[closest_pixel_index[0], closest_pixel_index[1], :]\n",
    "\n",
    "# Replace NaN values with the placeholder, and ensure data is an integer type\n",
    "tb4_flag_timeseries_nonan = np.nan_to_num(tb4_flag_timeseries, nan=nan_placeholder).astype(np.int64)\n",
    "\n",
    "# Initialize rfi_detected array with zeros (no RFI detected by default)\n",
    "rfi_detected = np.zeros(tb4_flag_timeseries_nonan.shape, dtype=float)\n",
    "\n",
    "# Check if RFI is detected (bitwise operation), and mark it as 1 in rfi_detected\n",
    "rfi_detected[(tb4_flag_timeseries_nonan & rfi_flag_mask) == rfi_flag_mask] = 1\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rfi_detected, label='RFI Detection Status', linestyle='-', marker='o', markersize=4)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('RFI Detection (1: Detected, 0: Not Detected)')\n",
    "plt.title(f'RFI Detection over Time for Pixel Closest to Seoul, Korea')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.1, 1.1)  # Set y-axis limits to make peaks more visible\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295470b-dbdc-416a-bdcd-6a62c65a55fb",
   "metadata": {},
   "source": [
    "# 1. Import SMOS_IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa894ba9-c292-4c65-b66a-e645bc8922c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this line if there is only tar files.\n",
    "root_dir = os.path.join(cpuserver_data_FP, 'SMOS_IC')\n",
    "\n",
    "#for year in [2015, 2022]:\n",
    "#    hSMOS_IC.extract_tgz_files(root_dir, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64f3ad-f072-433e-b18a-aa4800f89d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_list = ['A', 'D']\n",
    "for year in range(2022, 2023):  # Looping from 2015 to 2021\n",
    "    for path in path_list:\n",
    "        print(year, path)\n",
    "        directory = os.path.join(root_dir, path, str(year))\n",
    "        file_list, data_doy = hSMOS_IC.extract_filelist_doy(directory)\n",
    "\n",
    "        # Import Variables\n",
    "        SMOS_SM, longitude, latitude = hSMOS_IC.create_array_from_nc(file_list, data_doy, year, 'Soil_Moisture')\n",
    "        SMOS_SM[SMOS_SM<=0] = np.nan\n",
    "        SMOS_RFI = hSMOS_IC.create_array_from_nc(file_list, data_doy, year, 'Rfi_Prob')[0]\n",
    "        SMOS_SF  = hSMOS_IC.create_array_from_nc(file_list, data_doy, year, 'Science_Flags')[0]\n",
    "\n",
    "        # Create a NetCDF file\n",
    "        path_time = 'am' if path == 'A' else 'pm'\n",
    "        \n",
    "        file_name = f'SMOS_IC_{year}_{path_time}.nc'\n",
    "        nc_file = os.path.join(cpuserver_data_FP, nc_save_dir, file_name)\n",
    "                     \n",
    "        hSMOS_IC.create_netcdf_file(nc_file, latitude, longitude, SMOS_SM, SMOS_RFI, SMOS_SF)\n",
    "\n",
    "        print(f\"NetCDF file created: {nc_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c513788-e13d-4012-84aa-9ade1c4aa1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "nanmean_SMOS_SM  = np.nanmean(SMOS_SM, axis=2)\n",
    "nanmean_SMOS_RFI = np.nanmean(SMOS_RFI, axis=2)\n",
    "target = nanmean_SMOS_RFI\n",
    "Plot.plot_global_map(longitude, latitude, target, 'SM', 0, 1, 'jet_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7548f8-ca81-40e0-9774-4d861f7bfa32",
   "metadata": {},
   "source": [
    "# 2. Import ASCAT from mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa062f-3433-4278-8918-f992bc4a3d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = cpuserver_data_FP+'/mat_files/ASCAT_h119_120/'\n",
    "\n",
    "for year in range(2015, 2022):\n",
    "    path_list = ['am', 'pm']\n",
    "    for path in path_list:\n",
    "        mat_file = root_dir + str(year) + '_ASCAT_SM_' + path + '_QC.mat'\n",
    "        ASCAT_SM, latitude, longitude = hASCAT_TUW.load_mat_file(mat_file, path)\n",
    "        file_name = 'ASCAT_TUW_'+str(year)+'_'+path+'.nc'\n",
    "        print(file_name)\n",
    "        nc_file = os.path.join(cpuserver_data_FP, 'extracted_nc')+'/'+file_name\n",
    "        hASCAT_TUW.create_netcdf_file(nc_file, latitude, longitude, ASCAT_SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf4fd1-91fc-486b-9dce-0a59a2850bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nanmean_ASCAT_SM = np.nanmean(ASCAT_SM_am, axis=2)\n",
    "#target = nanmean_ASCAT_SM\n",
    "Plot.plot_global_map(longitude, latitude, target, 'SM', 0, 10000, 'jet_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa051d9-b775-4546-a06e-f65d7b5d161b",
   "metadata": {},
   "source": [
    "# 3. Resample SMOS and ASCAT data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba869fc-fb2a-4e15-ad14-2da6157a7149",
   "metadata": {},
   "source": [
    "### 3.1. Load the domain lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c12ce-cc50-4bfc-b3a6-2b2ea1b566b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#domain_nc_file = base_FP+'/Lab/2023_DA_TCA/libs/lis_input_noah33.nc'\n",
    "#nc_data = netCDF4.Dataset(domain_nc_file)\n",
    "#domain_lat = np.flipud(nc_data.variables['lat'][:]).data\n",
    "#domain_lon = np.flipud(nc_data.variables['lon'][:]).data\n",
    "#nc_data.close()\n",
    "\n",
    "mission_product = 'SPL3SMP.008'\n",
    "domain_lon, domain_lat = get_e2grid(cpuserver_data_FP, mission_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4047b-6128-4639-9bb3-32f59bf1d856",
   "metadata": {},
   "source": [
    "### 3.2. Resample SMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c2ed0-4acd-45ae-9ecd-1fdc4826ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2016, 2023):\n",
    "    path_list = ['am', 'pm']\n",
    "    \n",
    "    for path in path_list:\n",
    "        # resample SMOS into the doamin size\n",
    "        nc_file  = nc_save_dir+'/extracted_nc/SMOS_IC_'+str(year)+'_'+path+'.nc'\n",
    "        nc_data  = netCDF4.Dataset(nc_file)\n",
    "        SMOS_SM  = nc_data.variables['SMOS_SM'][:].data\n",
    "        SMOS_RFI = nc_data.variables['SMOS_RFI'][:].data\n",
    "        #SMOS_SF  = nc_data.variables['SMOS_SF'][:].data\n",
    "        SMOS_lat = nc_data.variables['latitude'][:].data\n",
    "        SMOS_lon = nc_data.variables['longitude'][:].data\n",
    "        nc_data.close()\n",
    "    \n",
    "        Resampled_SMOS_SM  = Data.Resampling_forloop(domain_lat, domain_lon, SMOS_lat, SMOS_lon, SMOS_SM)\n",
    "        Resampled_SMOS_RFI = Data.Resampling_forloop(domain_lat, domain_lon, SMOS_lat, SMOS_lon, SMOS_RFI)\n",
    "        #Resampled_SMOS_SF  = Data.Resampling_parallel(domain_lat, domain_lon, SMOS_lat, SMOS_lon, SMOS_SF, 'nearest', 'mode',3) #too slow\n",
    "        #We may need to apply RFI/SF first and resample SM\n",
    "        file_name = 'SMOS_IC_'+str(year)+'_'+path+'_R_'+mission_product+'.nc'\n",
    "        nc_file = os.path.join(nc_save_dir, 'extracted_nc')+'/'+file_name\n",
    "        hSMOS_IC.create_netcdf_file(nc_file, domain_lat, domain_lon, Resampled_SMOS_SM, Resampled_SMOS_RFI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f306c2-6c40-4526-acf4-decc226c8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Resampling fun in Data works\n",
    "doy = 100\n",
    "r_smos = Resampled_SMOS_SM[:,:,doy]\n",
    "Plot.plot_global_map(domain_lon, domain_lat, r_smos, 'or_SM', 0, 0.5, 'jet_r')\n",
    "smos = SMOS_SM[:,:,doy]\n",
    "Plot.plot_global_map(SMOS_lon, SMOS_lat, smos, 're_SM', 0, 0.5, 'jet_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0780f2-fb94-4196-8477-0a5a3ccde3a4",
   "metadata": {},
   "source": [
    "### 3.3. Resample ASCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f27a0e-7f64-4d25-b659-934ea734457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import porosity\n",
    "porosity_EASE9_mat_file  = cpuserver_data_FP+'/mat_files/soil_climate_etc/world_PORO_EASE_009.mat'\n",
    "ease_grid_EASE9_mat_file = cpuserver_data_FP+'/mat_files/ease_grid_files/ease_lat_lon_9km.mat'\n",
    "\n",
    "poro_EASE9 = hASCAT_TUW.load_porosity_mat(porosity_EASE9_mat_file)['PORO']\n",
    "lat_EASE9 = hASCAT_TUW.load_porosity_mat(ease_grid_EASE9_mat_file)['ease_lat_9km']\n",
    "lon_EASE9 = hASCAT_TUW.load_porosity_mat(ease_grid_EASE9_mat_file)['ease_lon_9km']\n",
    "\n",
    "Plot.plot_global_map(lon_EASE9, lat_EASE9, poro_EASE9, 'Porosity (-)', 0.3, 0.6, 'jet_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08923af6-7f64-467e-bde1-6f2541aa132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample SM\n",
    "for year in range(2015, 2023):\n",
    "    path_list = ['am', 'pm']\n",
    "    for path in path_list:\n",
    "        # resample ASCAT into the doamin size\n",
    "        nc_file  = nc_save_dir+'/extracted_nc/ASCAT_TUW_'+str(year)+'_'+path+'.nc'\n",
    "        nc_data  = netCDF4.Dataset(nc_file)\n",
    "        ASCAT_SM  = nc_data.variables['ASCAT_SM'][:].data\n",
    "        ASCAT_lat = nc_data.variables['latitude'][:].data\n",
    "        ASCAT_lon = nc_data.variables['longitude'][:].data\n",
    "        nc_data.close()\n",
    "        print('data_loaded')\n",
    "        Resampled_ASCAT_SM  = Data.Resampling_forloop(domain_lat, domain_lon, ASCAT_lat, ASCAT_lon, ASCAT_SM, 'nearest','mean', 1)\n",
    "        \n",
    "        file_name = 'ASCAT_IC_'+str(year)+'_'+path+'_R_'+mission_product+'.nc'\n",
    "        nc_file = os.path.join(nc_save_dir, 'extracted_nc')+'/'+file_name\n",
    "        hASCAT_TUW.create_netcdf_file(nc_file, domain_lat, domain_lon, Resampled_ASCAT_SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ff675-9d5c-4c3b-8105-eefb213adb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample porosity\n",
    "importlib.reload(hASCAT_TUW)\n",
    "Resampled_poro = Data.Resampling(domain_lat, domain_lon, lat_EASE9, lon_EASE9, poro_EASE9, sampling_method='nearest', agg_method='mean', mag_factor=2) \n",
    "nc_file  = nc_save_dir+'/extracted_nc/PORO_R_'+mission_product+'.nc'\n",
    "hASCAT_TUW.create_netcdf_file(nc_file, domain_lat, domain_lon, Resampled_poro, 'Porosity')\n",
    "\n",
    "r_poro = Resampled_poro\n",
    "Plot.plot_global_map(domain_lon, domain_lat, r_poro, 'r_Porosity (-)', 0.3, 0.6, 'jet_r')\n",
    "Plot.plot_global_map(lon_EASE9, lat_EASE9, poro_EASE9, 'or_Porosity (-)', 0.3, 0.6, 'jet_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dec3c3-45f3-4194-a491-3973ce44bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Resampling fun in Data works\n",
    "doy = 203\n",
    "r_ascat = Resampled_ASCAT_SM[:,:,doy]\n",
    "Plot.plot_global_map(domain_lon, domain_lat, r_ascat, 'r_SWI(-)', 0, 10000, 'jet_r')\n",
    "ascat = ASCAT_SM[:,:,doy]\n",
    "Plot.plot_global_map(ASCAT_lon, ASCAT_lat, ascat, 'or_SWI(-)', 0, 10000, 'jet_r')\n",
    "\n",
    "#np.nanmax(r_ascat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f8381-f093-43ee-b671-951dffd23ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "import datetime\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "\n",
    "if platform.system() == 'Darwin':\n",
    "    base_FP = '/Users/hyunglokkim/Insync/hkim@geol.sc.edu/Google_Drive'\n",
    "    sys.path.append(base_FP + '/python_modules')\n",
    "    nc_save_dir = '/Volumes/rapid'\n",
    "else:\n",
    "    base_FP = '/home/h/Insync/hkim@geol.sc.edu/Google Drive'\n",
    "    sys.path.append(base_FP + '/python_modules')\n",
    "    nc_save_dir = '/media/h/06BEFF93BEFF798F/data'\n",
    "\n",
    "import hydroAI.TC_like as TCL\n",
    "import hydroAI.Plot as Plot\n",
    "import hydroAI.Data as Data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd909f-2cf4-4af5-a471-32143d701f1a",
   "metadata": {},
   "source": [
    "### 4. Load resampled data, select time-period, and convert ASCAT SWI to VSM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823023e1-7009-45ff-a8c0-63def171ccce",
   "metadata": {},
   "source": [
    "### 4.1. Stduy domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60368c13-644d-41dd-b4fe-80799b478520",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_nc_file = base_FP+'/Lab/2023_DA_TCA/libs/lis_input_noah33.nc'\n",
    "nc_data = netCDF4.Dataset(domain_nc_file)\n",
    "domain_lat = np.flipud(nc_data.variables['lat'][:]).data\n",
    "domain_lon = np.flipud(nc_data.variables['lon'][:]).data\n",
    "nc_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb56d5-05bd-4dd0-8175-f53dd4ac1ef6",
   "metadata": {},
   "source": [
    "### 4.2. Specify the time-period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ee1c2-01e5-4cea-94ff-555477cc44ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.datetime(2022, 4, 1)\n",
    "end_date   = datetime.datetime(2022, 4, 30)\n",
    "\n",
    "# Calculate the day of year values\n",
    "doy1 = start_date.timetuple().tm_yday\n",
    "doy2 = end_date.timetuple().tm_yday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc4e65-a88a-47f2-864c-3ef8741c6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file_Porosity = nc_save_dir+'/extracted_nc/PORO_R_Noah_025.nc'\n",
    "nc_data_PORO   = netCDF4.Dataset(nc_file_Porosity)\n",
    "Porosity       = nc_data_PORO.variables['Porosity'][:].data\n",
    "\n",
    "m, n = Porosity.shape; z = doy2-doy1+1\n",
    "\n",
    "path_list = ['am', 'pm']\n",
    "RFI_th = 0.9\n",
    "\n",
    "#combined the am and pm data sets\n",
    "ASCAT_SM_am_pm = np.zeros((m, n, z*2), dtype='float64')\n",
    "SMOS_SM_am_pm  = np.zeros((m, n, z*2), dtype='float64')\n",
    "\n",
    "for path in path_list:\n",
    "    nc_file_SMOS   = nc_save_dir+'/extracted_nc/SMOS_IC_2022_'+path+'_R_Noah_025.nc'\n",
    "    nc_file_ASCAT  = nc_save_dir+'/extracted_nc/ASCAT_TUW_2022_'+path+'_R_Noah_025.nc'\n",
    "\n",
    "    nc_data_SMOS = netCDF4.Dataset(nc_file_SMOS)\n",
    "    SMOS_SM      = nc_data_SMOS.variables['SMOS_SM'][:].data\n",
    "    SMOS_RFI     = nc_data_SMOS.variables['SMOS_RFI'][:].data\n",
    "    SMOS_SM[SMOS_RFI>RFI_th] = np.nan\n",
    "    sub_SMOS_SM  = SMOS_SM[:,:, doy1:doy2+1]\n",
    "\n",
    "    nc_data_ASCAT = netCDF4.Dataset(nc_file_ASCAT)\n",
    "    ASCAT_SWI     = nc_data_ASCAT.variables['ASCAT_SM'][:].data\n",
    "    sub_ASCAT_SM  = ASCAT_SWI[:,:, doy1-1:doy2]/10000*np.expand_dims(Porosity, axis=2)\n",
    "\n",
    "    if path == 'am':\n",
    "        SMOS_SM_am_pm[:, :,  ::2]  = sub_SMOS_SM\n",
    "        ASCAT_SM_am_pm[:, :, ::2]  = sub_ASCAT_SM\n",
    "    else:\n",
    "        SMOS_SM_am_pm[:, :, 1::2]  = sub_SMOS_SM\n",
    "        ASCAT_SM_am_pm[:, :, 1::2] = sub_ASCAT_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321c69c-0f70-4938-bb70-c07d0d6a3dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ma_SMOS_SM_am_pm  = Data.moving_average_3d(SMOS_SM_am_pm, 5)\n",
    "ma_ASCAT_SM_am_pm = Data.moving_average_3d(ASCAT_SM_am_pm, 5)\n",
    "\n",
    "# pseudo DA_SM data. Generated from the ASCAT SM data with white noise.\n",
    "temp_error = np.random.normal(loc=0, scale=0.05, size=(m, n, z*2))\n",
    "DA_SM      = ASCAT_SM_am_pm + temp_error\n",
    "ma_DA_SM   = Data.moving_average_3d(DA_SM, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a95ae-26e1-422d-939b-561ebefe985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the moving-averaged SM vs originia SM data sets\n",
    "t1 = np.sum(ma_SMOS_SM_am_pm>0,2)\n",
    "t2 = np.sum(ma_ASCAT_SM_am_pm>0,2)\n",
    "row_indices, col_indices = np.where((t1 > 20) & (t2 > 20))\n",
    "idx = 1305\n",
    "\n",
    "plt.plot(ma_SMOS_SM_am_pm[row_indices[idx], col_indices[idx],:],'-o', label='ma_SMOS')\n",
    "plt.plot(SMOS_SM_am_pm[row_indices[idx], col_indices[idx],:], '-o', label='or_SMOS')\n",
    "\n",
    "plt.plot(ma_ASCAT_SM_am_pm[row_indices[idx], col_indices[idx],:],'-o', label='ma_ASCAT')\n",
    "plt.plot(ASCAT_SM_am_pm[row_indices[idx], col_indices[idx],:], '-o', label='or_ASCAT')\n",
    "\n",
    "plt.plot(ma_DA_SM[row_indices[idx], col_indices[idx],:],'-o', label='ma_DA_SM')\n",
    "plt.plot(DA_SM[row_indices[idx], col_indices[idx],:], '-o', label='or_DA_SM')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel(r'VSM ($\\frac{m^3}{m^3}$)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc704a5c-d012-42cf-9103-04f48cb86b2d",
   "metadata": {},
   "source": [
    "### 4.3 Calcuate the TC numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30565b4-4d86-4181-b4e1-34075a144a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = ma_SMOS_SM_am_pm\n",
    "D2 = ma_ASCAT_SM_am_pm\n",
    "D3 = ma_DA_SM\n",
    "fMSE = TCL.TCA(D1, D2, D3, nod_th=20, corr_th=0.1, REF=None)[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84edc85d-09f0-49af-94a4-b9c97a185397",
   "metadata": {},
   "source": [
    "### 4.3 Check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69433203-5c8a-489d-b839-970695bdd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOS_fMSE  = fMSE['x'] #the lower the better\n",
    "ASCAT_fMSE = fMSE['y'] #the lower the better\n",
    "DA_fMSE    = fMSE['z'] #the lower the better\n",
    "\n",
    "Plot.plot_global_map(domain_lon, domain_lat, SMOS_fMSE, 'SMOS fMSE(-)', 0, 1, 'jet')\n",
    "Plot.plot_global_map(domain_lon, domain_lat, ASCAT_fMSE, 'ASCAT fMSE(-)', 0, 1, 'jet')\n",
    "Plot.plot_global_map(domain_lon, domain_lat, DA_fMSE, 'DA fMSE(-)', 0, 1, 'jet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
